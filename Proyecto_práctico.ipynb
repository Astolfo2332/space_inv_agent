{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Actividad - Proyecto práctico\n",
    "\n",
    "\n",
    "> La actividad se desarrollará en grupos pre-definidos de 2-3 alumnos. Se debe indicar los nombres en orden alfabético (de apellidos). Recordad que esta actividad se corresponde con un 30% de la nota final de la asignatura. Se debe entregar entregar el trabajo en la presente notebook.\n",
    "*   Alumno 1:\n",
    "*   Alumno 2:\n",
    "*   Alumno 3:\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "jUehXgCyIRdq"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## **PARTE 1** - Instalación y requisitos previos\n",
    "\n",
    "> Las prácticas han sido preparadas para poder realizarse en el entorno de trabajo de Google Colab. Sin embargo, esta plataforma presenta ciertas incompatibilidades a la hora de visualizar la renderización en gym. Por ello, para obtener estas visualizaciones, se deberá trasladar el entorno de trabajo a local. Por ello, el presente dosier presenta instrucciones para poder trabajar en ambos entornos. Siga los siguientes pasos para un correcto funcionamiento:\n",
    "1.   **LOCAL:** Preparar el enviroment, siguiendo las intrucciones detalladas en la sección *1.1.Preparar enviroment*.\n",
    "2.  **AMBOS:** Modificar las variables \"mount\" y \"drive_mount\" a la carpeta de trabajo en drive en el caso de estar en Colab, y ejecturar la celda *1.2.Localizar entorno de trabajo*.\n",
    "3. **COLAB:** se deberá ejecutar las celdas correspondientes al montaje de la carpeta de trabajo en Drive. Esta corresponde a la sección *1.3.Montar carpeta de datos local*.\n",
    "4.  **AMBOS:** Instalar las librerías necesarias, siguiendo la sección *1.4.Instalar librerías necesarias*.\n"
   ],
   "metadata": {
    "id": "JwpYlnjWJhS9"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "### 1.1. Preparar enviroment (solo local)\n",
    "\n",
    "\n",
    "\n",
    "> Para preparar el entorno de trabajo en local, se han seguido los siguientes pasos:\n",
    "1. En Windows, puede ser necesario instalar las C++ Build Tools. Para ello, siga los siguientes pasos: https://towardsdatascience.com/how-to-install-openai-gym-in-a-windows-environment-338969e24d30.\n",
    "2. Instalar Anaconda\n",
    "3. Siguiendo el código que se presenta comentado en la próxima celda: Crear un enviroment, cambiar la ruta de trabajo, e instalar librerías básicas.\n",
    "\n",
    "\n",
    "```\n",
    "conda create --name miar_rl python=3.8\n",
    "conda activate miar_rl\n",
    "cd \"PATH_TO_FOLDER\"\n",
    "conda install git\n",
    "pip install jupyter\n",
    "```\n",
    "\n",
    "\n",
    "4. Abrir la notebook con *jupyter-notebook*.\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "jupyter-notebook\n",
    "```\n"
   ],
   "metadata": {
    "id": "RU2BPrK2JkP0"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "### 1.2. Localizar entorno de trabajo: Google colab o local"
   ],
   "metadata": {
    "id": "w-kixNPiJqTc"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from networkx.tests.test_all_random_functions import progress\n",
    "from sympy import print_rcode\n",
    "\n",
    "# ATENCIÓN!! Modificar ruta relativa a la práctica si es distinta (drive_root)\n",
    "mount='/content/gdrive'\n",
    "drive_root = mount + \"/My Drive/08_MIAR/actividades/proyecto practico\"\n",
    "\n",
    "try:\n",
    "  from google.colab import drive\n",
    "  IN_COLAB=True\n",
    "except:\n",
    "  IN_COLAB=False"
   ],
   "metadata": {
    "id": "S_YDFwZ-JscI"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "### 1.3. Montar carpeta de datos local (solo Colab)"
   ],
   "metadata": {
    "id": "8Dp_a1iBJ0tf"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Switch to the directory on the Google Drive that you want to use\n",
    "import os\n",
    "if IN_COLAB:\n",
    "  print(\"We're running Colab\")\n",
    "\n",
    "  if IN_COLAB:\n",
    "    # Mount the Google Drive at mount\n",
    "    print(\"Colab: mounting Google drive on \", mount)\n",
    "\n",
    "    drive.mount(mount)\n",
    "\n",
    "    # Create drive_root if it doesn't exist\n",
    "    create_drive_root = True\n",
    "    if create_drive_root:\n",
    "      print(\"\\nColab: making sure \", drive_root, \" exists.\")\n",
    "      os.makedirs(drive_root, exist_ok=True)\n",
    "\n",
    "    # Change to the directory\n",
    "    print(\"\\nColab: Changing directory to \", drive_root)\n",
    "    %cd $drive_root\n",
    "# Verify we're in the correct working directory\n",
    "%pwd\n",
    "print(\"Archivos en el directorio: \")\n",
    "print(os.listdir())"
   ],
   "metadata": {
    "id": "I6n7MIefJ21i"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "### 1.4. Instalar librerías necesarias"
   ],
   "metadata": {
    "id": "i1ZSL5bpJ560"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "if IN_COLAB:\n",
    "  %pip install gym==0.17.3\n",
    "  %pip install git+https://github.com/Kojoley/atari-py.git\n",
    "  %pip install keras-rl2==1.0.5\n",
    "  %pip install tensorflow==2.8\n",
    "else:\n",
    "  %pip install gym==0.17.3\n",
    "  %pip install git+https://github.com/Kojoley/atari-py.git\n",
    "  %pip install pyglet==1.5.0\n",
    "  %pip install h5py==3.1.0\n",
    "  %pip install Pillow==9.5.0\n",
    "  %pip install keras-rl2==1.0.5\n",
    "  %pip install Keras==2.2.4\n",
    "  %pip install tensorflow==2.5.3\n",
    "  %pip install torch==2.0.1\n",
    "  %pip install agents==1.4.0"
   ],
   "metadata": {
    "id": "UbVRjvHCJ8UF"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4hzP_5ZuGb2X"
   },
   "source": [
    "---\n",
    "## **PARTE 2**. Enunciado\n",
    "\n",
    "Consideraciones a tener en cuenta:\n",
    "\n",
    "- El entorno sobre el que trabajaremos será _SpaceInvaders-v0_ y el algoritmo que usaremos será _DQN_.\n",
    "\n",
    "- Para nuestro ejercicio, el requisito mínimo será alcanzado cuando el agente consiga una **media de recompensa por encima de 20 puntos en modo test**. Por ello, esta media de la recompensa se calculará a partir del código de test en la última celda del notebook.\n",
    "\n",
    "Este proyecto práctico consta de tres partes:\n",
    "\n",
    "1.   Implementar la red neuronal que se usará en la solución\n",
    "2.   Implementar las distintas piezas de la solución DQN\n",
    "3.   Justificar la respuesta en relación a los resultados obtenidos\n",
    "\n",
    "**Rúbrica**: Se valorará la originalidad en la solución aportada, así como la capacidad de discutir los resultados de forma detallada. El requisito mínimo servirá para aprobar la actividad, bajo premisa de que la discusión del resultado sera apropiada.\n",
    "\n",
    "IMPORTANTE:\n",
    "\n",
    "* Si no se consigue una puntuación óptima, responder sobre la mejor puntuación obtenida.\n",
    "* Para entrenamientos largos, recordad que podéis usar checkpoints de vuestros modelos para retomar los entrenamientos. En este caso, recordad cambiar los parámetros adecuadamente (sobre todo los relacionados con el proceso de exploración).\n",
    "* Se deberá entregar unicamente el notebook y los pesos del mejor modelo en un fichero .zip, de forma organizada.\n",
    "* Cada alumno deberá de subir la solución de forma individual."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## **PARTE 3**. Desarrollo y preguntas"
   ],
   "metadata": {
    "id": "6_b3mzw8IzJP"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "duPmUNOVGb2a"
   },
   "source": [
    "#### Importar librerías"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "j3eRhgI-Gb2a",
    "ExecuteTime": {
     "end_time": "2025-06-20T19:07:19.139508Z",
     "start_time": "2025-06-20T19:07:16.582421Z"
    }
   },
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import mlflow\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import os\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecFrameStack, VecNormalize, VecEnvWrapper\n",
    "from stable_baselines3.common.vec_env.vec_transpose import VecTransposeImage\n",
    "from stable_baselines3.common.callbacks import ProgressBarCallback, BaseCallback\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "import gymnasium as gym\n",
    "import ale_py\n",
    "\n",
    "from torchvision.models import mobilenet_v2\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import mlflow\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "from stable_baselines3.common.atari_wrappers import AtariWrapper\n",
    "\n",
    "from sb3_contrib import QRDQN"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f4jgQjzoGb2a"
   },
   "source": [
    "\n",
    "#### Configuración base"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-20T19:07:20.726921Z",
     "start_time": "2025-06-20T19:07:20.623606Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "INPUT_SHAPE = (84, 84)\n",
    "WINDOW_LENGTH = 4\n",
    "gym.register_envs(ale_py)\n",
    "class CustomPenaltyWrapper(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.last_action = None\n",
    "        self.same_action_count = 0\n",
    "        self.last_lives = None\n",
    "        self.no_shoot_count = 0\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        obs, info = self.env.reset(**kwargs)\n",
    "        self.last_action = None\n",
    "        self.same_action_count = 0\n",
    "        self.no_shoot_count = 0\n",
    "\n",
    "        if \"lives\" in info:\n",
    "            self.last_lives = info[\"lives\"]\n",
    "        else:\n",
    "            self.last_lives = None\n",
    "\n",
    "        return obs, info\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "\n",
    "        # --- Penalty for repeated no-op\n",
    "        if action == 0:\n",
    "            if self.last_action == 0:\n",
    "                self.same_action_count += 1\n",
    "            else:\n",
    "                self.same_action_count = 1\n",
    "        else:\n",
    "            self.same_action_count = 0\n",
    "        self.last_action = action\n",
    "\n",
    "        if self.same_action_count >= 3:\n",
    "            reward -= 1.0\n",
    "\n",
    "        # --- Penalty for getting hit\n",
    "        if \"lives\" in info and self.last_lives is not None:\n",
    "            if info[\"lives\"] < self.last_lives:\n",
    "                reward -= 0.5\n",
    "            self.last_lives = info[\"lives\"]\n",
    "\n",
    "        # --- Penalty for not shooting for too long\n",
    "        if int(action) in [1, 4, 5]:  # shooting actions\n",
    "            self.no_shoot_count = 0\n",
    "        else:\n",
    "            self.no_shoot_count += 1\n",
    "            if self.no_shoot_count >= 3:\n",
    "                reward -= 0.7\n",
    "\n",
    "        reward = np.clip(reward, -1.0, 1.0)\n",
    "\n",
    "        return obs, reward, terminated, truncated, info\n",
    "\n",
    "\n",
    "class NormalizeInput(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=0.0, high=1.0,\n",
    "            shape=self.observation_space.shape,\n",
    "            dtype=np.float32\n",
    "        )\n",
    "\n",
    "    def observation(self, observation):\n",
    "        return observation.astype(np.float32) / 255.0\n",
    "\n",
    "#Se usa la version sin skip de frames ya que el AtariWrapper lo hace\n",
    "env_name = 'SpaceInvadersNoFrameskip-v4'\n",
    "env = gym.make(env_name)\n",
    "#En este caso el AtariWrapper hace lo mismo que Clipreward además de añadir el preprocesado de las imágenes\n",
    "#Normalizamos las imágenes a 0-1\n",
    "# env = NormalizeInput(env)\n",
    "env = AtariWrapper(env)\n",
    "normal_env = NormalizeInput(env)\n",
    "pen_env = CustomPenaltyWrapper(normal_env)  # Añadimos el wrapper de penalización\n",
    "\n",
    "#Train env con penalizaciones\n",
    "env = Monitor(pen_env)\n",
    "env = DummyVecEnv([lambda: env])  # Convertimos a un entorno vectorizado\n",
    "# # # Se crea el entorno de vectores y se apilan los frames\n",
    "env = VecFrameStack(env, 4)\n",
    "\n",
    "#Test env sin penalizaciones\n",
    "normal_env = Monitor(normal_env)\n",
    "normal_env = DummyVecEnv([lambda: normal_env])  # Convertimos a un entorno vectorizado\n",
    "# # # Se crea el entorno de vectores y se apilan los frames\n",
    "normal_env = VecFrameStack(normal_env, 4)\n",
    "\n",
    "np.random.seed(123)\n",
    "obs = env.reset()\n",
    "nb_actions = env.action_space.n\n",
    "# print(env.shape)\n",
    "print(obs.shape)\n",
    "print(nb_actions)\n",
    "print(\"maximo de altura\", max(obs[0, :, 0, :].flatten()))\n",
    "print(\"maximo de ancho\", max(obs[0, 0 :, :].flatten()))\n",
    "\n",
    "# print(\"maximo de altura\", max(obs[:, 0, :].flatten()))\n",
    "# print(\"maximo de ancho\", max(obs[0, :, :].flatten()))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 84, 84, 4)\n",
      "6\n",
      "maximo de altura 0.30980393\n",
      "maximo de ancho 0.52156866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.11.1+2750686)\n",
      "[Powered by Stella]\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "normal_env.unwrapped.get_action_meanings()",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-20T16:28:37.438202Z",
     "start_time": "2025-06-20T16:28:37.355356Z"
    }
   },
   "cell_type": "code",
   "source": [
    "obs, info = pen_env.reset()\n",
    "done = False\n",
    "total_reward = 0\n",
    "\n",
    "while not done:\n",
    "    action = np.random.choice([0, 2, 3])\n",
    "    obs, reward, terminated, truncated, info = pen_env.step(action)\n",
    "    total_reward += reward\n",
    "    if terminated or truncated:\n",
    "        done = True\n",
    "\n",
    "    print(\"Reward:\", reward)\n",
    "print(f\"Total reward: {total_reward}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward: 0.0\n",
      "Reward: 0.0\n",
      "Reward: -0.7\n",
      "Reward: -0.7\n",
      "Reward: -0.7\n",
      "Reward: -0.7\n",
      "Reward: -0.7\n",
      "Reward: -0.7\n",
      "Reward: -0.7\n",
      "Reward: -0.7\n",
      "Reward: -0.7\n",
      "Reward: -0.7\n",
      "Reward: -0.7\n",
      "Reward: -0.7\n",
      "Reward: -0.7\n",
      "Reward: -0.7\n",
      "Reward: -0.7\n",
      "Reward: -0.7\n",
      "Reward: -0.7\n",
      "Reward: -0.7\n",
      "Reward: -0.7\n",
      "Reward: -0.7\n",
      "Reward: -0.7\n",
      "Reward: -0.7\n",
      "Reward: -0.7\n",
      "Reward: -0.7\n",
      "Reward: -0.7\n",
      "Reward: -0.7\n",
      "Reward: -0.7\n",
      "Reward: -0.7\n",
      "Reward: -0.7\n",
      "Reward: -0.7\n",
      "Reward: -0.7\n",
      "Reward: -0.7\n",
      "Reward: -0.7\n",
      "Reward: -0.7\n",
      "Reward: -0.7\n",
      "Reward: -0.7\n",
      "Reward: -0.7\n",
      "Reward: -0.7\n",
      "Reward: -0.7\n",
      "Reward: -0.7\n",
      "Reward: -0.7\n",
      "Reward: -0.7\n",
      "Reward: -0.7\n",
      "Reward: -0.7\n",
      "Reward: -0.7\n",
      "Reward: -0.7\n",
      "Reward: -0.7\n",
      "Reward: -0.7\n",
      "Reward: -0.7\n",
      "Reward: -0.7\n",
      "Reward: -0.7\n",
      "Reward: -0.7\n",
      "Reward: -0.7\n",
      "Reward: -0.7\n",
      "Reward: -0.7\n",
      "Reward: -0.7\n",
      "Reward: -0.7\n",
      "Reward: -0.7\n",
      "Reward: -0.7\n",
      "Reward: -0.7\n",
      "Reward: -0.7\n",
      "Reward: -0.7\n",
      "Reward: -0.7\n",
      "Reward: -0.7\n",
      "Reward: -0.7\n",
      "Reward: -0.7\n",
      "Total reward: -46.20000000000004\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-20T16:44:08.350821Z",
     "start_time": "2025-06-20T16:44:08.288932Z"
    }
   },
   "cell_type": "code",
   "source": [
    "obs, info = pen_env.reset()\n",
    "done = False\n",
    "total_reward = 0\n",
    "\n",
    "while not done:\n",
    "    action = 0\n",
    "    obs, reward, terminated, truncated, info = pen_env.step(action)\n",
    "    total_reward += reward\n",
    "    if terminated or truncated:\n",
    "        done = True\n",
    "\n",
    "    print(\"Reward:\", reward)\n",
    "print(f\"Total reward: {total_reward}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward: 0.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Total reward: -48.0\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7yitXTADGb2b"
   },
   "source": "1. Implementación de la red neuronal"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "O4GKrfWSGb2b"
   },
   "source": [
    "class MobileNetFeatureExtractor(BaseFeaturesExtractor):\n",
    "    def __init__(self, observation_space, features_dim=256):\n",
    "        super().__init__(observation_space, features_dim)\n",
    "\n",
    "        # Pretrained MobileNetV2 without the classifier\n",
    "        weights = torchvision.models.MobileNet_V2_Weights.DEFAULT\n",
    "        self.backbone = torchvision.models.mobilenet_v2(weights=weights)\n",
    "\n",
    "        # Freeze weights (optional)\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Compute shape by doing one forward pass\n",
    "        with torch.no_grad():\n",
    "            sample = torch.as_tensor(observation_space.sample()[None]).float()\n",
    "            if sample.shape[1] != 3:  # Convert grayscale to 3 channels\n",
    "                sample = sample.repeat(1, 3, 1, 1)\n",
    "            n_flatten = self.backbone(sample).view(sample.shape[0], -1).shape[1]\n",
    "\n",
    "        self.projector = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(n_flatten, n_flatten // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(n_flatten // 2, n_flatten // 3),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(n_flatten // 3, features_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, obs):\n",
    "        # Convert 1-channel grayscale to 3 channels if needed\n",
    "        if obs.shape[1] == 1:\n",
    "            obs = obs.repeat(1, 3, 1, 1)\n",
    "        features = self.backbone(obs)\n",
    "        return self.projector(features)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Convirtiendo las imágenes a 3 canales"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class VitB16FeaturesExtractor(BaseFeaturesExtractor):\n",
    "    def __init__(self, observation_space, features_dim=256):\n",
    "        super().__init__(observation_space, features_dim)\n",
    "\n",
    "        # Load a pretrained ViT model\n",
    "        weights = torchvision.models.ViT_B_16_Weights.DEFAULT\n",
    "        self.backbone = torchvision.models.vit_b_16(weights=weights).cuda()\n",
    "\n",
    "        # Freeze weights (optional)\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Compute shape by doing one forward pass\n",
    "        with torch.no_grad():\n",
    "            sample = torch.as_tensor(observation_space.sample()[None]).float()\n",
    "            if sample.shape[1] != 3:  # Convert grayscale to 3 channels\n",
    "                sample = sample.repeat(1, 3, 1, 1)\n",
    "            sample = self._preprocess(sample).cuda()\n",
    "            n_flatten = self.backbone(sample).view(sample.shape[0], -1).shape[1]\n",
    "\n",
    "        self.projector = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(n_flatten, features_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def _preprocess(self, observation):\n",
    "        # Preprocess the observation to match the input requirements of ViT\n",
    "        batch_resize = F.interpolate(\n",
    "            observation, size=(224, 224), mode='bilinear', align_corners=False\n",
    "        )\n",
    "        return batch_resize\n",
    "\n",
    "\n",
    "    def forward(self, obs):\n",
    "        # Convert 1-channel grayscale to 3 channels if needed\n",
    "        obs = self._preprocess(obs).cuda()\n",
    "        if obs.shape[1] == 1:\n",
    "            obs = obs.repeat(1, 3, 1, 1)\n",
    "        features = self.backbone(obs)\n",
    "        return self.projector(features)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Escala de grises"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class VitB16FeaturesExtractor(BaseFeaturesExtractor):\n",
    "    def __init__(self, observation_space, features_dim=256):\n",
    "        super().__init__(observation_space, features_dim)\n",
    "\n",
    "        self.chanel_mapper = nn.Conv2d(\n",
    "            in_channels=1, out_channels=3, kernel_size=1, stride=1, padding=0\n",
    "        )\n",
    "        self.uppscaler = nn.Upsample(size=(224, 224), mode='bilinear', align_corners=False)\n",
    "        # Load a pretrained ViT model\n",
    "        weights = torchvision.models.ViT_B_16_Weights.DEFAULT\n",
    "        self.backbone = torchvision.models.vit_b_16(weights=weights).cuda()\n",
    "\n",
    "        # Freeze weights (optional)\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Compute shape by doing one forward pass\n",
    "        with torch.no_grad():\n",
    "            sample = torch.as_tensor(observation_space.sample()[None]).float()\n",
    "            if sample.shape[1] != 3:  # Convert grayscale to 3 channels\n",
    "                sample = sample.repeat(1, 3, 1, 1)\n",
    "            sample = self._preprocess(sample).cuda()\n",
    "            n_flatten = self.backbone(sample).view(sample.shape[0], -1).shape[1]\n",
    "\n",
    "        self.projector = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(n_flatten, features_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def _preprocess(self, observation):\n",
    "        # Preprocess the observation to match the input requirements of ViT\n",
    "        batch_resize = F.interpolate(\n",
    "            observation, size=(224, 224), mode='bilinear', align_corners=False\n",
    "        )\n",
    "        return batch_resize\n",
    "\n",
    "\n",
    "    def forward(self, obs):\n",
    "        # Convert 1-channel grayscale to 3 channels if needed\n",
    "        # obs = self._preprocess(obs).cuda()\n",
    "        obs = self.chanel_mapper(obs).cuda()\n",
    "        obs = self.uppscaler(obs).cuda()\n",
    "        # if obs.shape[1] == 1:\n",
    "        #     obs = obs.repeat(1, 3, 1, 1)\n",
    "        features = self.backbone(obs)\n",
    "        return self.projector(features)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class ResNet152FeaturesExtractor(BaseFeaturesExtractor):\n",
    "    def __init__(self, observation_space, features_dim=256):\n",
    "        super().__init__(observation_space, features_dim)\n",
    "\n",
    "        # Load a pretrained ResNet152 model\n",
    "        weights = torchvision.models.ResNet152_Weights.DEFAULT\n",
    "        self.backbone = torchvision.models.resnet152(weights=weights)\n",
    "\n",
    "        # Freeze weights (optional)\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Compute shape by doing one forward pass\n",
    "        with torch.no_grad():\n",
    "            sample = torch.as_tensor(observation_space.sample()[None]).float()\n",
    "            if sample.shape[1] != 3:  # Convert grayscale to 3 channels\n",
    "                sample = sample.repeat(1, 3, 1, 1)\n",
    "            n_flatten = self.backbone(sample).view(sample.shape[0], -1).shape[1]\n",
    "\n",
    "        self.projector = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(n_flatten, features_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, obs):\n",
    "        # Convert 1-channel grayscale to 3 channels if needed\n",
    "        if obs.shape[1] == 1:\n",
    "            obs = obs.repeat(1, 3, 1, 1)\n",
    "        features = self.backbone(obs)\n",
    "        return self.projector(features)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### CNNConnectedDeep\n",
    "\n",
    "- Prueba inicial con reescalado y en escala de grises."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class CNNConnectedDeep(BaseFeaturesExtractor):\n",
    "    def __init__(self, observation_space, features_dim=256):\n",
    "        super().__init__(observation_space, features_dim)\n",
    "\n",
    "        # Primeras capas\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(32, 32, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.dropout = nn.Dropout(0.4)\n",
    "\n",
    "        # Concatenación de conv1 y conv3\n",
    "        # 32 (resized conv1) + 64 = 96\n",
    "        # Bloques de compresión adicionales con conexiones\n",
    "        self.conv4 = nn.Conv2d(96, 128, kernel_size=3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(128)\n",
    "\n",
    "        self.conv5 = nn.Conv2d(128, 160, kernel_size=3, padding=1)\n",
    "        self.bn5 = nn.BatchNorm2d(160)\n",
    "\n",
    "        self.conv6 = nn.Conv2d(160, 192, kernel_size=3, padding=1)\n",
    "        self.bn6 = nn.BatchNorm2d(192)\n",
    "\n",
    "        self.conv7 = nn.Conv2d(192 + 128, 224, kernel_size=3, padding=1)  # concat con out4\n",
    "        self.bn7 = nn.BatchNorm2d(224)\n",
    "\n",
    "        self.conv8 = nn.Conv2d(224 + 160, 256, kernel_size=3, padding=1)  # concat con out5\n",
    "        self.bn8 = nn.BatchNorm2d(256)\n",
    "\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d((4, 4))  # reduce a [B, 256, 4, 4]\n",
    "\n",
    "        # Flatten: 256 * 4 * 4 = 4096 → muy alto → reducimos\n",
    "        self.fc1 = nn.Linear(256 * 4 * 4, 512)\n",
    "        self.fc2 = nn.Linear(512, features_dim)\n",
    "\n",
    "    def _preprocess(self, observation):\n",
    "        r, g, b = observation[:, 0:1, :, :], observation[:, 1:2, :, :], observation[:, 2:3, :, :]\n",
    "        gray = 0.2989 * r + 0.5870 * g + 0.1140 * b\n",
    "        gray3 = gray.repeat(1, 3, 1, 1)  # Convert to 3 channels\n",
    "\n",
    "        batch_resize = F.interpolate(\n",
    "            gray3, size=(224, 224), mode='bilinear', align_corners=False\n",
    "        )\n",
    "        return batch_resize\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self._preprocess(x)  # Convert grayscale to 3 channels and resize to 224x224\n",
    "        out1 = F.relu(self.bn1(self.conv1(x)))  # [B, 32, 224, 224]\n",
    "        out2 = F.relu(self.bn2(self.conv2(out1)))\n",
    "        out2 = out1 + out2  # Residual connection\n",
    "        out2 = self.pool(out2)  # [B, 32, 112, 112]\n",
    "\n",
    "        out3 = F.relu(self.bn3(self.conv3(out2)))\n",
    "        out3 = self.pool(out3)  # [B, 64, 56, 56]\n",
    "\n",
    "        out1_resized = F.interpolate(out1, size=out3.shape[2:])\n",
    "        concat1 = torch.cat((out3, out1_resized), dim=1)  # [B, 96, 56, 56]\n",
    "\n",
    "        # Bloque 4\n",
    "        out4 = F.relu(self.bn4(self.conv4(concat1)))\n",
    "        out4 = self.pool(out4)  # [B, 128, 28, 28]\n",
    "\n",
    "        # Bloque 5\n",
    "        out5 = F.relu(self.bn5(self.conv5(out4)))\n",
    "        out5 = self.pool(out5)  # [B, 160, 14, 14]\n",
    "\n",
    "        # Bloque 6\n",
    "        out6 = F.relu(self.bn6(self.conv6(out5)))\n",
    "        out6 = self.pool(out6)  # [B, 192, 7, 7]\n",
    "\n",
    "        # Concat out4 (resized) con out6\n",
    "        out4_resized = F.interpolate(out4, size=out6.shape[2:])\n",
    "        concat2 = torch.cat((out6, out4_resized), dim=1)  # [B, 192+128=320, 7, 7]\n",
    "        out7 = F.relu(self.bn7(self.conv7(concat2)))\n",
    "\n",
    "        # Concat out5 (resized) con out7\n",
    "        out5_resized = F.interpolate(out5, size=out7.shape[2:])\n",
    "        concat3 = torch.cat((out7, out5_resized), dim=1)  # [B, 224+160=384, 7, 7]\n",
    "        out8 = F.relu(self.bn8(self.conv8(concat3)))\n",
    "\n",
    "        x = self.global_pool(out8)  # [B, 256, 4, 4]\n",
    "        x = x.view(x.size(0), -1)   # Flatten → [B, 4096]\n",
    "\n",
    "        x = self.dropout(F.relu(self.fc1(x)))  # 4096 → 512\n",
    "        out = self.fc2(x)  # 512 → num_classes\n",
    "\n",
    "        return out\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- Adaptación a nuevo tamaño de red 84,84"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class CNNConnectedDeep(BaseFeaturesExtractor):\n",
    "    def __init__(self, observation_space, features_dim=256):\n",
    "        super().__init__(observation_space, features_dim)\n",
    "\n",
    "        # Primeras capas\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(32, 32, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.dropout = nn.Dropout(0.4)\n",
    "\n",
    "        # Concatenación de conv1 y conv3\n",
    "        # 32 (resized conv1) + 64 = 96\n",
    "        # Bloques de compresión adicionales con conexiones\n",
    "        self.conv4 = nn.Conv2d(96, 128, kernel_size=3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(128)\n",
    "\n",
    "        self.conv5 = nn.Conv2d(128, 160, kernel_size=3, padding=1)\n",
    "        self.bn5 = nn.BatchNorm2d(160)\n",
    "\n",
    "        self.conv6 = nn.Conv2d(160, 192, kernel_size=3, padding=1)\n",
    "        self.bn6 = nn.BatchNorm2d(192)\n",
    "\n",
    "        self.conv7 = nn.Conv2d(192 + 128, 224, kernel_size=3, padding=1)  # concat con out4\n",
    "        self.bn7 = nn.BatchNorm2d(224)\n",
    "\n",
    "        self.conv8 = nn.Conv2d(224 + 160, 256, kernel_size=3, padding=1)  # concat con out5\n",
    "        self.bn8 = nn.BatchNorm2d(256)\n",
    "\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d((4, 4))  # reduce a [B, 256, 4, 4]\n",
    "\n",
    "        # Flatten: 256 * 4 * 4 = 4096 → muy alto → reducimos\n",
    "        self.fc1 = nn.Linear(256 * 4 * 4, 512)\n",
    "        self.fc2 = nn.Linear(512, features_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out1 = F.relu(self.bn1(self.conv1(x)))  # [B, 32, 224, 224]\n",
    "        out2 = F.relu(self.bn2(self.conv2(out1)))\n",
    "        out2 = out1 + out2  # Residual connection\n",
    "        out2 = self.pool(out2)  # [B, 32, 112, 112]\n",
    "\n",
    "        out3 = F.relu(self.bn3(self.conv3(out2)))\n",
    "        out3 = self.pool(out3)  # [B, 64, 56, 56]\n",
    "\n",
    "        out1_resized = F.interpolate(out1, size=out3.shape[2:])\n",
    "        concat1 = torch.cat((out3, out1_resized), dim=1)  # [B, 96, 56, 56]\n",
    "\n",
    "        # Bloque 4\n",
    "        out4 = F.relu(self.bn4(self.conv4(concat1)))\n",
    "        out4 = self.pool(out4)  # [B, 128, 28, 28]\n",
    "\n",
    "        # Bloque 5\n",
    "        out5 = F.relu(self.bn5(self.conv5(out4)))\n",
    "        out5 = self.pool(out5)  # [B, 160, 14, 14]\n",
    "\n",
    "        # Bloque 6\n",
    "        out6 = F.relu(self.bn6(self.conv6(out5)))\n",
    "        out6 = self.pool(out6)  # [B, 192, 7, 7]\n",
    "\n",
    "        # Concat out4 (resized) con out6\n",
    "        out4_resized = F.interpolate(out4, size=out6.shape[2:])\n",
    "        concat2 = torch.cat((out6, out4_resized), dim=1)  # [B, 192+128=320, 7, 7]\n",
    "        out7 = F.relu(self.bn7(self.conv7(concat2)))\n",
    "\n",
    "        # Concat out5 (resized) con out7\n",
    "        out5_resized = F.interpolate(out5, size=out7.shape[2:])\n",
    "        concat3 = torch.cat((out7, out5_resized), dim=1)  # [B, 224+160=384, 7, 7]\n",
    "        out8 = F.relu(self.bn8(self.conv8(concat3)))\n",
    "\n",
    "        x = self.global_pool(out8)  # [B, 256, 4, 4]\n",
    "        x = x.view(x.size(0), -1)   # Flatten → [B, 4096]\n",
    "\n",
    "        x = self.dropout(F.relu(self.fc1(x)))  # 4096 → 512\n",
    "        out = self.fc2(x)  # 512 → num_classes\n",
    "\n",
    "        return out\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### DeepMind"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-20T19:07:53.538617Z",
     "start_time": "2025-06-20T19:07:53.535024Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class DeepMindCNN(BaseFeaturesExtractor):\n",
    "    \"\"\"\n",
    "    DeepMind-style CNN used in the original DQN paper (Mnih et al., 2015).\n",
    "    Input shape: (n_stack, 84, 84) → (4, 84, 84)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, observation_space, features_dim=512):\n",
    "        # features_dim is the output of the last linear layer (fc1)\n",
    "        super().__init__(observation_space, features_dim)\n",
    "\n",
    "        # Check input shape\n",
    "        n_input_channels = observation_space.shape[2]  # e.g., 4 stacked grayscale frames\n",
    "\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(n_input_channels, 32, kernel_size=8, stride=4),  # (32, 20, 20)\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),                 # (64, 9, 9)\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),                 # (64, 7, 7)\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            sample_input = torch.as_tensor(observation_space.sample()[None]).float()\n",
    "            sample_input = self._preprocess(sample_input)  # Preprocess the input\n",
    "            n_flatten = self.cnn(sample_input).shape[1]\n",
    "\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(n_flatten , n_flatten // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(n_flatten // 2, n_flatten // 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(n_flatten // 4, features_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def _preprocess(self, x):\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        return x\n",
    "\n",
    "    def forward(self, observations: torch.Tensor) -> torch.Tensor:\n",
    "        x = self._preprocess(observations)\n",
    "        x = self.cnn(x)\n",
    "        return self.linear(x)"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Dueling"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from stable_baselines3.dqn.policies import DQNPolicy\n",
    "class DuelingCnnExtractor(BaseFeaturesExtractor):\n",
    "    def __init__(self, observation_space, features_dim=512):\n",
    "        super().__init__(observation_space, features_dim)\n",
    "        n_input_channels = observation_space.shape[0]\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(n_input_channels, 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            n_flatten = self.cnn(torch.zeros(1, *observation_space.shape)).shape[1]\n",
    "\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(n_flatten, features_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, obs):\n",
    "        return self.linear(self.cnn(obs))\n",
    "\n",
    "class DuelingDQNPolicy(DQNPolicy):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(\n",
    "            *args,\n",
    "            features_extractor_class=DuelingCnnExtractor,\n",
    "            **kwargs\n",
    "        )\n",
    "        # Rebuild Q network with dueling architecture\n",
    "        features_dim = self.q_net.q_net[0].in_features\n",
    "        action_dim = self.action_space.n\n",
    "\n",
    "        self.q_net = nn.Sequential(\n",
    "            nn.Linear(features_dim, 512),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.q_net_adv = nn.Linear(512, action_dim)\n",
    "        self.q_net_val = nn.Linear(512, 1)\n",
    "\n",
    "    def forward(self, obs, deterministic=False):\n",
    "        features = self.extract_features(obs, features_extractor=DuelingCnnExtractor)\n",
    "        x = self.q_net(features)\n",
    "        adv = self.q_net_adv(x)\n",
    "        val = self.q_net_val(x)\n",
    "        q_values = val + adv - adv.mean(dim=1, keepdim=True)\n",
    "        return q_values"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OB9-_5HPGb2b"
   },
   "source": [
    "2. Implementación de la solución DQN"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Nota**: Las primeras pruebas fueron sin usar el AtariWrapper, pero se preprocesaba internamente en las capas."
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-20T19:07:56.584143Z",
     "start_time": "2025-06-20T19:07:56.580961Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TQDMProgressCallback(BaseCallback):\n",
    "    def __init__(self, total_timesteps: int, verbose=0, inital=None):\n",
    "        super().__init__(verbose)\n",
    "        self.total_timesteps = total_timesteps\n",
    "        self.progress_bar = None\n",
    "        self.inital = inital\n",
    "\n",
    "    def _on_training_start(self):\n",
    "        if self.inital is None:\n",
    "            self.progress_bar = tqdm(total=self.total_timesteps, desc=\"Training Progress\", unit=\"step\")\n",
    "        else:\n",
    "            self.progress_bar = tqdm(total=self.total_timesteps, initial=self.inital, desc=\"Training Progress\", unit=\"step\")\n",
    "\n",
    "    def _on_step(self):\n",
    "        self.progress_bar.update(1)\n",
    "        # Optional: log latest reward if available\n",
    "        infos = self.locals.get(\"infos\", [])\n",
    "        if infos and isinstance(infos[0], dict) and \"episode\" in infos[0]:\n",
    "            self.progress_bar.set_postfix(reward=infos[0][\"episode\"][\"r\"])\n",
    "        return True  # Return True to continue training\n",
    "\n",
    "    def _on_training_end(self):\n",
    "        self.progress_bar.close()"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-20T19:07:58.227806Z",
     "start_time": "2025-06-20T19:07:58.223Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MLflowCallback(BaseCallback):\n",
    "    def __init__(self, best_model_path, experiment_name=\"SB3_Experiment\", run_name=None, log_freq=1000, verbose=0, save_freq=100_000):\n",
    "        super().__init__(verbose)\n",
    "        self.experiment_name = experiment_name\n",
    "        self.log_freq = log_freq\n",
    "        self.step_count = 0\n",
    "        self.best_mean_reward = -np.inf\n",
    "        self.best_model_path = best_model_path\n",
    "        self.save_freq = save_freq\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        self.step_count = self.num_timesteps\n",
    "        # print(self.step_count % self.log_freq)\n",
    "        if self.step_count % self.log_freq == 0:\n",
    "            rewards = [ep_info['r'] for ep_info in self.model.ep_info_buffer] if self.model.ep_info_buffer else []\n",
    "            lengths = [ep_info['l'] for ep_info in self.model.ep_info_buffer] if self.model.ep_info_buffer else []\n",
    "\n",
    "            mean_reward = np.mean(rewards) if rewards else 0.0\n",
    "            max_reward = np.max(rewards) if rewards else 0.0\n",
    "            min_reward = np.min(rewards) if rewards else 0.0\n",
    "            mean_length = np.mean(lengths) if lengths else 0.0\n",
    "            std_reward = np.std(rewards) if rewards else 0.0\n",
    "            exploration_mean = self.model.exploration_rate\n",
    "            loss_mean = self.logger.name_to_value.get(\"train/loss\", 0)\n",
    "            training_updates = self.logger.name_to_value.get(\"train/n_updates\", 0)\n",
    "\n",
    "            step = self.num_timesteps\n",
    "            mlflow.log_metric(\"timesteps\", step, step=step)\n",
    "            mlflow.log_metric(\"episode_reward_mean\", mean_reward, step=step)\n",
    "            mlflow.log_metric(\"episode_reward_max\", max_reward, step=step)\n",
    "            mlflow.log_metric(\"episode_reward_min\", min_reward, step=step)\n",
    "            mlflow.log_metric(\"episode_length_mean\", mean_length, step=step)\n",
    "            mlflow.log_metric(\"episode_reward_std\", std_reward, step=step)\n",
    "            mlflow.log_metric(\"episode_length_std\", std_reward, step=step)\n",
    "            mlflow.log_metric(\"exploration_rate\", exploration_mean, step=step)\n",
    "            if loss_mean != 0:\n",
    "                mlflow.log_metric(\"loss_mean\", loss_mean, step=step)\n",
    "            if training_updates != 0:\n",
    "                mlflow.log_metric(\"training_updates\", training_updates, step=step)\n",
    "\n",
    "            if mean_reward > self.best_mean_reward and self.save_freq % self.step_count == 0:\n",
    "                self.best_mean_reward = mean_reward\n",
    "                # Save the best model\n",
    "                self.model.save(self.best_model_path)\n",
    "        if self.step_count % 1_000_000 == 0:\n",
    "            # Log the model as an artifact\n",
    "            self.model.save(self.best_model_path.replace(\".zip\", \"_lastest.zip\"))\n",
    "\n",
    "        return True\n",
    "\n",
    "    def _on_training_end(self):\n",
    "        # Optionally save the model as artifact\n",
    "        mlflow.log_param(\"num_episodes\", len(self.model.ep_info_buffer))\n",
    "        mlflow.end_run()"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-20T19:08:00.102981Z",
     "start_time": "2025-06-20T19:08:00.099141Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from collections import Counter\n",
    "\n",
    "class TestCallBack(BaseCallback):\n",
    "    def __init__(self, env, n_episodes=100, verbose=0, test_timesteps=10000):\n",
    "        super().__init__(verbose)\n",
    "        self.env = env\n",
    "        self.n_episodes = n_episodes\n",
    "        self.rewards = []\n",
    "        self.test_timesteps = test_timesteps\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.num_timesteps % self.test_timesteps == 0:  # Test every 1000 steps\n",
    "            action_counter = Counter()\n",
    "            for _ in range(self.n_episodes):\n",
    "                ep_reward = 0\n",
    "                obs = self.env.reset()\n",
    "                done = False\n",
    "                while not done:\n",
    "                    with torch.no_grad():\n",
    "                        action, _ = self.model.predict(obs)\n",
    "                    obs, reward, done, _ = self.env.step(action)\n",
    "                    action_scalar = int(action)\n",
    "                    action_counter[action_scalar] += 1\n",
    "                    ep_reward += reward\n",
    "                self.rewards.append(ep_reward)\n",
    "            mean_reward = np.mean(self.rewards)\n",
    "            std_reward = np.std(self.rewards)\n",
    "            mlflow.log_metric(\"test_reward\", mean_reward, step=self.num_timesteps)\n",
    "            mlflow.log_metric(\"test_reward_std\", std_reward, step=self.num_timesteps)\n",
    "            total_actions = sum(action_counter.values())\n",
    "            for action, count in action_counter.items():\n",
    "                mlflow.log_metric(f\"action_{action}_count\", count, step=self.num_timesteps)\n",
    "                mlflow.log_metric(f\"action_{action}_percentage\", count / total_actions, step=self.num_timesteps)\n",
    "        return True"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-20T19:08:03.192573Z",
     "start_time": "2025-06-20T19:08:03.187918Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from collections import deque\n",
    "class AdaptiveEpsilon:\n",
    "    def __init__(\n",
    "        self,\n",
    "        epsilon=0.05,\n",
    "        min_epsilon=0.05,\n",
    "        max_epsilon=0.3,\n",
    "        stagnation_threshold=0.5,\n",
    "        patience=100,\n",
    "        increase_factor=1.1,\n",
    "        decay_factor=0.85,\n",
    "        stagnation_limit=1000\n",
    "    ):\n",
    "        self.min_epsilon = min_epsilon\n",
    "        self.max_epsilon = max_epsilon\n",
    "        self.epsilon = epsilon\n",
    "        self.patience = patience\n",
    "        self.stagnation_threshold = stagnation_threshold\n",
    "        self.increase_factor = increase_factor\n",
    "        self.decay_factor = decay_factor\n",
    "        self.stagnation_limit = stagnation_limit\n",
    "\n",
    "        self.best_mean_reward = -np.inf\n",
    "        self.steps_since_improvement = 0\n",
    "\n",
    "    def update(self, reward_history):\n",
    "        if len(reward_history) < reward_history.maxlen:\n",
    "            return self.epsilon\n",
    "\n",
    "        current_mean = np.mean(reward_history)\n",
    "        if current_mean > self.best_mean_reward + self.stagnation_threshold:\n",
    "            self.best_mean_reward = current_mean\n",
    "            self.steps_since_improvement = 0\n",
    "        else:\n",
    "            self.steps_since_improvement += 1\n",
    "\n",
    "        if self.steps_since_improvement > self.stagnation_limit:\n",
    "            self.epsilon = min(self.max_epsilon, self.epsilon * self.increase_factor)\n",
    "        elif self.steps_since_improvement > self.patience:\n",
    "            self.epsilon = min(self.max_epsilon, self.epsilon * 1.1)\n",
    "        else:\n",
    "            self.epsilon = max(self.min_epsilon, self.epsilon * self.decay_factor)\n",
    "\n",
    "        return self.epsilon\n",
    "\n",
    "class EpsilonCallback(BaseCallback):\n",
    "    def __init__(self, adaptive_eps: AdaptiveEpsilon, reward_history: deque, verbose=0):\n",
    "        super().__init__(verbose)\n",
    "        self.adaptive_eps = adaptive_eps\n",
    "        self.reward_history = reward_history\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.num_timesteps < self.model.learning_starts:\n",
    "            return True\n",
    "\n",
    "        if self.num_timesteps % 100 == 0:\n",
    "            rewards = [ep_info['r'] for ep_info in self.model.ep_info_buffer] if self.model.ep_info_buffer else []\n",
    "            if not rewards:\n",
    "                return True  # Skip if no rewards available\n",
    "            self.reward_history.append(np.mean(rewards))\n",
    "            return True\n",
    "\n",
    "        if self.num_timesteps % 10000 != 0:\n",
    "            return True  # Skip if not a multiple of 1000\n",
    "\n",
    "        new_eps = self.adaptive_eps.update(self.reward_history)\n",
    "        self.model.exploration_rate = new_eps\n",
    "        self.model.exploration_schedule = lambda _: new_eps  # Update exploration schedule\n",
    "        if self.verbose > 0:\n",
    "            print(f\"Updated epsilon: {new_eps:.4f}\")\n",
    "        return True"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Resnet152"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "policy_kwargs = dict(\n",
    "    features_extractor_class=ResNet152FeaturesExtractor,\n",
    "    features_extractor_kwargs=dict(features_dim=256),\n",
    ")\n",
    "total_timesteps = 100000\n",
    "progress_bar_callback = TQDMProgressCallback(total_timesteps=total_timesteps)\n",
    "ml_callback = MLflowCallback(\n",
    "    \"models/dqn_resnet152_weights.zip\",\n",
    "    experiment_name=\"DQN_SpaceInvaders\",\n",
    "    run_name=\"DQN_Run\",\n",
    "    log_freq=500\n",
    ")\n",
    "experiment_name = \"DQN_SpaceInvaders\"\n",
    "exist_experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "if not exist_experiment:\n",
    "    mlflow.create_experiment(experiment_name)\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "with mlflow.start_run(run_name=\"DQN_Run_ResNet152_finetuned\"):\n",
    "    mlflow.log_param(\"env_name\", env_name)\n",
    "    mlflow.log_param(\"total_timesteps\", total_timesteps)\n",
    "    mlflow.log_param(\"learning_rate\", 1e-4)\n",
    "    mlflow.log_param(\"buffer_size\", 100000)\n",
    "\n",
    "    model = DQN(\"CnnPolicy\", env, verbose=1, learning_rate=1e-4, buffer_size=100000, policy_kwargs=policy_kwargs)\n",
    "    t_model = model.learn(total_timesteps=total_timesteps, callback=[progress_bar_callback, ml_callback, TestCallBack(env)])\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### MobileNetV2"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "foSlxWH1Gb2b"
   },
   "source": [
    "policy_kwargs = dict(\n",
    "    features_extractor_class=MobileNetFeatureExtractor,\n",
    "    features_extractor_kwargs=dict(features_dim=256),\n",
    ")\n",
    "total_timesteps = 2_000_000\n",
    "progress_bar_callback = TQDMProgressCallback(total_timesteps=total_timesteps)\n",
    "ml_callback = MLflowCallback(\n",
    "    \"models/dqn_mobilenet_v2_weights_new_data.zip\",\n",
    "    experiment_name=\"DQN_SpaceInvaders\",\n",
    "    run_name=\"DQN_Run\",\n",
    "    log_freq=10_000\n",
    ")\n",
    "experiment_name = \"DQN_SpaceInvaders\"\n",
    "exist_experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "if not exist_experiment:\n",
    "    mlflow.create_experiment(experiment_name)\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "with mlflow.start_run(run_name=\"DQN_Run_MobileNetv2_finetuned_new_data\"):\n",
    "    mlflow.log_param(\"env_name\", env_name)\n",
    "    mlflow.log_param(\"total_timesteps\", total_timesteps)\n",
    "    mlflow.log_param(\"learning_rate\", 1e-4)\n",
    "    mlflow.log_param(\"buffer_size\", 500_000)\n",
    "\n",
    "    model = DQN(\"CnnPolicy\", env, verbose=0, learning_rate=1e-4, buffer_size=500_000, policy_kwargs=policy_kwargs)\n",
    "    t_model = model.learn(total_timesteps=total_timesteps, callback=[progress_bar_callback, ml_callback, TestCallBack(env, test_timesteps=100_000)])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### ViT B-16"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Cargar pesos preentrenados"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "total_timesteps = 100000\n",
    "progress_bar_callback = TQDMProgressCallback(total_timesteps=total_timesteps)\n",
    "ml_callback = MLflowCallback(\n",
    "    best_model_path=\"models/dqn_vit_b_16_weights.zip\",\n",
    "    experiment_name=\"DQN_SpaceInvaders\",\n",
    "    run_name=\"DQN_Run\",\n",
    "    log_freq=500\n",
    ")\n",
    "experiment_name = \"DQN_SpaceInvaders\"\n",
    "exist_experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "if not exist_experiment:\n",
    "    mlflow.create_experiment(experiment_name)\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "with mlflow.start_run(run_name=\"DQN_Run_Vit_b_16_finetuned\"):\n",
    "    mlflow.log_param(\"env_name\", env_name)\n",
    "    mlflow.log_param(\"total_timesteps\", total_timesteps)\n",
    "    mlflow.log_param(\"learning_rate\", 1e-4)\n",
    "    mlflow.log_param(\"buffer_size\", 100000)\n",
    "    model = DQN.load(\"models/dqn_vit_b_16_weights.zip\", env=env)\n",
    "    t_model = model.learn(total_timesteps=total_timesteps, callback=[progress_bar_callback, ml_callback])\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "policy_kwargs = dict(\n",
    "    features_extractor_class=VitB16FeaturesExtractor,\n",
    "    features_extractor_kwargs=dict(features_dim=256),\n",
    ")\n",
    "total_timesteps = 100000\n",
    "progress_bar_callback = TQDMProgressCallback(total_timesteps=total_timesteps)\n",
    "ml_callback = MLflowCallback(\n",
    "    best_model_path=\"models/dqn_vit_b_16_weights_upscaled.zip\",\n",
    "    experiment_name=\"DQN_SpaceInvaders\",\n",
    "    run_name=\"DQN_Run\",\n",
    "    log_freq=500\n",
    ")\n",
    "experiment_name = \"DQN_SpaceInvaders\"\n",
    "exist_experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "if not exist_experiment:\n",
    "    mlflow.create_experiment(experiment_name)\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "with mlflow.start_run(run_name=\"DQN_Run_Vit_b_16_gray_upscaled\"):\n",
    "    mlflow.log_param(\"env_name\", env_name)\n",
    "    mlflow.log_param(\"total_timesteps\", total_timesteps)\n",
    "    mlflow.log_param(\"learning_rate\", 1e-4)\n",
    "    mlflow.log_param(\"buffer_size\", 100000)\n",
    "\n",
    "    model = DQN(\"CnnPolicy\", env, verbose=0, learning_rate=1e-4, buffer_size=100000, policy_kwargs=policy_kwargs)\n",
    "    t_model = model.learn(total_timesteps=total_timesteps, callback=[progress_bar_callback, ml_callback, TestCallBack(env)])\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### CNNConnectedDeep"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Por el momento es el mejor modelo por su entrenamiento y resultados iniciales, además de velocidad."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "policy_kwargs = dict(\n",
    "    features_extractor_class=CNNConnectedDeep,\n",
    "    features_extractor_kwargs=dict(features_dim=256)\n",
    ")\n",
    "total_timesteps = 1_000_000\n",
    "progress_bar_callback = TQDMProgressCallback(total_timesteps=total_timesteps)\n",
    "ml_callback = MLflowCallback(\n",
    "    best_model_path=\"models/dqn_cnn_connected_deep_weights.zip\",\n",
    "    experiment_name=\"DQN_SpaceInvaders\",\n",
    "    run_name=\"DQN_Run\",\n",
    "    log_freq=20_000\n",
    ")\n",
    "experiment_name = \"DQN_SpaceInvaders\"\n",
    "exist_experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "if not exist_experiment:\n",
    "    mlflow.create_experiment(experiment_name)\n",
    "mlflow.set_experiment(experiment_name)\n",
    "with mlflow.start_run(run_name=\"DQN_Run_CNNConnectedDeep\"):\n",
    "    mlflow.log_param(\"env_name\", env_name)\n",
    "    mlflow.log_param(\"total_timesteps\", total_timesteps)\n",
    "    mlflow.log_param(\"learning_rate\", 25e-5)\n",
    "    mlflow.log_param(\"buffer_size\", 1_000_000)\n",
    "    mlflow.log_param(\"exploration_fraction\", 0.1)\n",
    "    mlflow.log_param(\"exploration_final_eps\", 0.05)\n",
    "    mlflow.log_param(\"exploration_initial_eps\", 1.0)\n",
    "    mlflow.log_param(\"batch_size\", 32)\n",
    "    mlflow.log_param(\"gamma\", 0.95)\n",
    "    mlflow.log_param(\"learning_starts\", 10_000)\n",
    "    mlflow.log_param(\"target_update_interval\", 10000)\n",
    "    mlflow.log_param(\"file_name\", \"dqn_cnn_connected_deep_weights.zip\")\n",
    "\n",
    "    model = DQN(\"CnnPolicy\", env, verbose=0,\n",
    "                policy_kwargs=policy_kwargs,\n",
    "                learning_rate=25e-5, buffer_size=1_000_000,\n",
    "                exploration_fraction=0.1, exploration_initial_eps=1.0,\n",
    "                exploration_final_eps=0.05, target_update_interval=10_00,\n",
    "                batch_size=32, learning_starts=10_000, gamma=0.95,seed=23)\n",
    "\n",
    "    t_model = model.learn(total_timesteps=total_timesteps, callback=[progress_bar_callback, ml_callback, TestCallBack(env, test_timesteps=40_000)], log_interval=9999)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3. Fine tuning de los modelos"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### CCNConnectedDeep\n",
    "con nuevas dimensiones de 84,84"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "policy_kwargs = dict(\n",
    "    features_extractor_class=CNNConnectedDeep,\n",
    "    features_extractor_kwargs=dict(features_dim=256)\n",
    ")\n",
    "total_timesteps = 5_000_000\n",
    "progress_bar_callback = TQDMProgressCallback(total_timesteps=total_timesteps)\n",
    "ml_callback = MLflowCallback(\n",
    "    best_model_path=\"models/dqn_cnn_connected_deep_weights_finetuning.zip\",\n",
    "    experiment_name=\"DQN_SpaceInvaders\",\n",
    "    run_name=\"DQN_Run\",\n",
    "    log_freq=25000\n",
    ")\n",
    "\n",
    "experiment_name = \"DQN_SpaceInvaders\"\n",
    "exist_experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "if not exist_experiment:\n",
    "    mlflow.create_experiment(experiment_name)\n",
    "mlflow.set_experiment(experiment_name)\n",
    "with mlflow.start_run(run_name=\"DQN_Run_CNNConnectedDeep_finetuned\"):\n",
    "    mlflow.log_param(\"env_name\", env_name)\n",
    "    mlflow.log_param(\"total_timesteps\", total_timesteps)\n",
    "    mlflow.log_param(\"learning_rate\", 1e-3)\n",
    "    mlflow.log_param(\"buffer_size\", 100_000)\n",
    "    mlflow.log_param(\"exploration_fraction\", 0.1)\n",
    "    mlflow.log_param(\"exploration_final_eps\", 0.1)\n",
    "    mlflow.log_param(\"exploration_initial_eps\", 1.0)\n",
    "    mlflow.log_param(\"batch_size\", 32)\n",
    "    mlflow.log_param(\"gamma\", 0.99)\n",
    "    mlflow.log_param(\"learning_starts\", 10000)\n",
    "    mlflow.log_param(\"target_update_interval\", 20000)\n",
    "\n",
    "    model = DQN(\"CnnPolicy\", env, verbose=0,\n",
    "                learning_rate=1e-4, buffer_size=300_000,\n",
    "                policy_kwargs=policy_kwargs,\n",
    "                exploration_fraction=0.3, exploration_initial_eps=1.0,\n",
    "                exploration_final_eps=0.01, target_update_interval=20000,\n",
    "                batch_size=32, learning_starts=100_000, gamma=0.99,seed=23)\n",
    "    t_model = model.learn(total_timesteps=total_timesteps, callback=[progress_bar_callback, ml_callback, TestCallBack(env, test_timesteps=50_000)])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### DeppMind\n",
    "\n",
    "- Ya viene por efecto en stable-Baselines3"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "policy_kwargs = dict(\n",
    "    net_arch = [256, 256],\n",
    ")\n",
    "total_timesteps = 100000\n",
    "progress_bar_callback = TQDMProgressCallback(total_timesteps=total_timesteps)\n",
    "ml_callback = MLflowCallback(\n",
    "    best_model_path=\"models/deep_mind_data.zip\",\n",
    "    experiment_name=\"DQN_SpaceInvaders\",\n",
    "    run_name=\"DQN_Run\",\n",
    "    log_freq=1000\n",
    ")\n",
    "experiment_name = \"DQN_SpaceInvaders\"\n",
    "exist_experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "if not exist_experiment:\n",
    "    mlflow.create_experiment(experiment_name)\n",
    "mlflow.set_experiment(experiment_name)\n",
    "with mlflow.start_run(run_name=\"deep_mind\"):\n",
    "    mlflow.log_param(\"env_name\", env_name)\n",
    "    mlflow.log_param(\"total_timesteps\", total_timesteps)\n",
    "    mlflow.log_param(\"learning_rate\", 1e-4)\n",
    "    mlflow.log_param(\"buffer_size\", 100000)\n",
    "\n",
    "    model = DQN(\"CnnPolicy\", env, verbose=0, learning_rate=1e-4, buffer_size=100000, policy_kwargs=policy_kwargs)\n",
    "    t_model = model.learn(total_timesteps=total_timesteps, callback=[progress_bar_callback, ml_callback, TestCallBack(env, test_timesteps=5000)])\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# old_model = DQN.load(\"models/deep_mind_more_train.zip\", env=env)\n",
    "\n",
    "policy_kwargs = dict(\n",
    "    net_arch = [256, 256],\n",
    ")\n",
    "\n",
    "total_timesteps = 20_000_000\n",
    "progress_bar_callback = TQDMProgressCallback(total_timesteps=total_timesteps)\n",
    "ml_callback = MLflowCallback(\n",
    "    best_model_path=\"models/deep_mind_finetuning.zip\",\n",
    "    experiment_name=\"DQN_SpaceInvaders\",\n",
    "    run_name=\"DQN_Run\",\n",
    "    log_freq=100_000\n",
    ")\n",
    "\n",
    "experiment_name = \"DQN_SpaceInvaders\"\n",
    "exist_experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "if not exist_experiment:\n",
    "    mlflow.create_experiment(experiment_name)\n",
    "mlflow.set_experiment(experiment_name)\n",
    "with mlflow.start_run(run_name=\"DQN_Run_deep_mind_finetuned\"):\n",
    "    mlflow.log_param(\"env_name\", env_name)\n",
    "    mlflow.log_param(\"total_timesteps\", total_timesteps)\n",
    "    mlflow.log_param(\"learning_rate\", 25e-5)\n",
    "    mlflow.log_param(\"buffer_size\", 1_000_000)\n",
    "    mlflow.log_param(\"exploration_fraction\", 0.1)\n",
    "    mlflow.log_param(\"exploration_final_eps\", 0.05)\n",
    "    mlflow.log_param(\"exploration_initial_eps\", 1.0)\n",
    "    mlflow.log_param(\"batch_size\", 32)\n",
    "    mlflow.log_param(\"gamma\", 0.95)\n",
    "    mlflow.log_param(\"learning_starts\", 10000)\n",
    "    mlflow.log_param(\"target_update_interval\", 10000)\n",
    "    mlflow.log_param(\"file_name\", \"deep_mind_finetuning.zip\")\n",
    "\n",
    "    model = DQN(\"CnnPolicy\", env, verbose=0,\n",
    "                policy_kwargs=policy_kwargs,\n",
    "                learning_rate=25e-5, buffer_size=1_000_000,\n",
    "                exploration_fraction=0.1, exploration_initial_eps=1.0,\n",
    "                exploration_final_eps=0.05, target_update_interval=10000,\n",
    "                batch_size=32, learning_starts=100_000, gamma=0.95,seed=23)\n",
    "    # model.policy.load_state_dict(old_model.policy.state_dict())\n",
    "\n",
    "    t_model = model.learn(total_timesteps=total_timesteps, callback=[progress_bar_callback, ml_callback, TestCallBack(env, test_timesteps=200_000)])\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model = DQN.load(\"models/deep_mind_finetuning\", env=env)\n",
    "model.exploration_initial_eps = 0\n",
    "model.exploration_rate = 0\n",
    "model.learning_starts = 500_000\n",
    "model.exploration_fraction = 0\n",
    "\n",
    "initial_steps = model.num_timesteps\n",
    "\n",
    "total_timesteps = 20_000_000\n",
    "ml_callback = MLflowCallback(\n",
    "    best_model_path=\"models/deep_mind_finetuning_continue.zip\",\n",
    "    experiment_name=\"DQN_SpaceInvaders\",\n",
    "    run_name=\"DQN_Run\",\n",
    "    log_freq=100_000\n",
    ")\n",
    "progress_bar_callback = TQDMProgressCallback(total_timesteps=total_timesteps, inital=initial_steps)\n",
    "\n",
    "experiment_name = \"DQN_SpaceInvaders\"\n",
    "exist_experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "if not exist_experiment:\n",
    "    mlflow.create_experiment(experiment_name)\n",
    "mlflow.set_experiment(experiment_name)\n",
    "with mlflow.start_run(run_name=\"DQN_Run_deep_mind_finetuned\"):\n",
    "    mlflow.log_param(\"env_name\", env_name)\n",
    "    mlflow.log_param(\"total_timesteps\", total_timesteps)\n",
    "    mlflow.log_param(\"learning_rate\", 25e-5)\n",
    "    mlflow.log_param(\"buffer_size\", 1_000_000)\n",
    "    mlflow.log_param(\"exploration_fraction\", 0.1)\n",
    "    mlflow.log_param(\"exploration_final_eps\", 0.05)\n",
    "    mlflow.log_param(\"exploration_initial_eps\", 1.0)\n",
    "    mlflow.log_param(\"batch_size\", 32)\n",
    "    mlflow.log_param(\"gamma\", 0.95)\n",
    "    mlflow.log_param(\"learning_starts\", 10000)\n",
    "    mlflow.log_param(\"target_update_interval\", 10000)\n",
    "    mlflow.log_param(\"file_name\", \"deep_mind_finetuning_continue.zip\")\n",
    "    t_model = model.learn(total_timesteps=total_timesteps, callback=[progress_bar_callback, ml_callback, TestCallBack(env, test_timesteps=200_000)], reset_num_timesteps=False, log_interval=49_999)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Tuning con dueling"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "policy_kwargs = dict(\n",
    "    net_arch = [256, 256],\n",
    ")\n",
    "total_timesteps = 100_000\n",
    "progress_bar_callback = TQDMProgressCallback(total_timesteps=total_timesteps)\n",
    "ml_callback = MLflowCallback(\n",
    "    best_model_path=\"models/duel.zip\",\n",
    "    experiment_name=\"DQN_SpaceInvaders\",\n",
    "    run_name=\"DQN_Run\",\n",
    "    log_freq=1000\n",
    ")\n",
    "\n",
    "experiment_name = \"DQN_SpaceInvaders\"\n",
    "exist_experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "if not exist_experiment:\n",
    "    mlflow.create_experiment(experiment_name)\n",
    "mlflow.set_experiment(experiment_name)\n",
    "with mlflow.start_run(run_name=\"duel\"):\n",
    "    mlflow.log_param(\"env_name\", env_name)\n",
    "    mlflow.log_param(\"total_timesteps\", total_timesteps)\n",
    "    mlflow.log_param(\"learning_rate\", 1e-4)\n",
    "    model = QRDQN(\"CnnPolicy\", env, verbose=0, learning_rate=1e-4, buffer_size=100000, policy_kwargs=policy_kwargs, seed=23)\n",
    "\n",
    "    t_model = model.learn(total_timesteps=total_timesteps, callback=[progress_bar_callback, ml_callback, TestCallBack(env, test_timesteps=10_000)])\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## DeepMind\n",
    "\n",
    "- Se carga el modelo luego de 5M de pasos con más contexto"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "old_model = DQN.load(\"models/deep_mind_finetuning.zip\", env=env)\n",
    "\n",
    "policy_kwargs = dict(\n",
    "    net_arch = [256, 256],\n",
    ")\n",
    "\n",
    "total_timesteps = 5_000_000\n",
    "progress_bar_callback = TQDMProgressCallback(total_timesteps=total_timesteps)\n",
    "ml_callback = MLflowCallback(\n",
    "    best_model_path=\"models/deep_mind_finetuning_more.zip\",\n",
    "    experiment_name=\"DQN_SpaceInvaders\",\n",
    "    run_name=\"DQN_Run\",\n",
    "    log_freq=25000\n",
    ")\n",
    "\n",
    "experiment_name = \"DQN_SpaceInvaders\"\n",
    "exist_experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "if not exist_experiment:\n",
    "    mlflow.create_experiment(experiment_name)\n",
    "mlflow.set_experiment(experiment_name)\n",
    "with mlflow.start_run(run_name=\"DQN_Run_deep_mind_finetuned_more\"):\n",
    "    mlflow.log_param(\"env_name\", env_name)\n",
    "    mlflow.log_param(\"total_timesteps\", total_timesteps)\n",
    "    mlflow.log_param(\"learning_rate\", 1e-3)\n",
    "    mlflow.log_param(\"buffer_size\", 1_000_000)\n",
    "    mlflow.log_param(\"exploration_fraction\", 0.2)\n",
    "    mlflow.log_param(\"exploration_final_eps\", 0.01)\n",
    "    mlflow.log_param(\"exploration_initial_eps\", 1.0)\n",
    "    mlflow.log_param(\"batch_size\", 32)\n",
    "    mlflow.log_param(\"gamma\", 0.99)\n",
    "    mlflow.log_param(\"learning_starts\", 100_000)\n",
    "    mlflow.log_param(\"target_update_interval\", 1_000)\n",
    "\n",
    "    model = DQN(\"CnnPolicy\", env, verbose=0,\n",
    "                policy_kwargs=policy_kwargs,\n",
    "                learning_rate=1e-3, buffer_size=1_000_000,\n",
    "                exploration_fraction=0.1, exploration_initial_eps=0.05,\n",
    "                exploration_final_eps=0.01, target_update_interval=1000,\n",
    "                batch_size=32, learning_starts=100_000, gamma=0.99,seed=23)\n",
    "    model.policy.load_state_dict(old_model.policy.state_dict())\n",
    "\n",
    "    t_model = model.learn(total_timesteps=total_timesteps, callback=[progress_bar_callback, ml_callback, TestCallBack(env, test_timesteps=50_000)])\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Custom DeepMind"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-06-20T19:08:41.382849Z"
    }
   },
   "cell_type": "code",
   "source": [
    "policy_kwargs = dict(\n",
    "    features_extractor_class=DeepMindCNN,\n",
    "    features_extractor_kwargs=dict(features_dim=512),\n",
    "    net_arch=[512, 216]\n",
    ")\n",
    "\n",
    "total_timesteps = 1_000_000\n",
    "progress_bar_callback = TQDMProgressCallback(total_timesteps=total_timesteps)\n",
    "ml_callback = MLflowCallback(\n",
    "    best_model_path=\"models/custom_DeepMind_penalized_v2.zip\",\n",
    "    experiment_name=\"DQN_SpaceInvaders\",\n",
    "    run_name=\"DQN_Run\",\n",
    "    log_freq=20_000\n",
    ")\n",
    "\n",
    "experiment_name = \"DQN_SpaceInvaders\"\n",
    "exist_experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "if not exist_experiment:\n",
    "    mlflow.create_experiment(experiment_name)\n",
    "mlflow.set_experiment(experiment_name)\n",
    "with mlflow.start_run(run_name=\"DQN_Run_custom_deepMind_penalized\"):\n",
    "    mlflow.log_param(\"env_name\", env_name)\n",
    "    mlflow.log_param(\"total_timesteps\", total_timesteps)\n",
    "    mlflow.log_param(\"learning_rate\", 25e-4)\n",
    "    mlflow.log_param(\"buffer_size\", 400_000)\n",
    "    mlflow.log_param(\"exploration_fraction\", 0.1)\n",
    "    mlflow.log_param(\"exploration_final_eps\", 0.05)\n",
    "    mlflow.log_param(\"exploration_initial_eps\", 1.0)\n",
    "    mlflow.log_param(\"batch_size\", 32)\n",
    "    mlflow.log_param(\"gamma\", 0.95)\n",
    "    mlflow.log_param(\"learning_starts\", 10000)\n",
    "    mlflow.log_param(\"target_update_interval\", 10000)\n",
    "    mlflow.log_param(\"file_name\", \"deep_mind_penalized_v2.zip\")\n",
    "    model = DQN(\"CnnPolicy\", env, verbose=0,\n",
    "                policy_kwargs=policy_kwargs,\n",
    "                learning_rate=25e-4, buffer_size=400_000,\n",
    "                exploration_fraction=0.1, exploration_initial_eps=1.0,\n",
    "                exploration_final_eps=0.09, target_update_interval=10000,\n",
    "                batch_size=32, learning_starts=10_000, gamma=0.95,seed=23)\n",
    "\n",
    "    t_model = model.learn(total_timesteps=total_timesteps, callback=[progress_bar_callback, ml_callback, TestCallBack(normal_env, test_timesteps=40_000)], log_interval=2_000)\n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Training Progress:   0%|          | 0/1000000 [00:00<?, ?step/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2e6b50358cfd4076a232882cd01d98bf"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-20T16:14:05.488430Z",
     "start_time": "2025-06-20T13:33:55.382778Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_old = DQN.load(\"models/custom_DeepMind_continue.zip\", env=env)\n",
    "\n",
    "policy_kwargs = dict(\n",
    "    features_extractor_class=DeepMindCNN,\n",
    "    features_extractor_kwargs=dict(features_dim=512),\n",
    "    net_arch=[512, 216]\n",
    ")\n",
    "\n",
    "reward_history = deque(maxlen=100)\n",
    "adaptive_eps = AdaptiveEpsilon()\n",
    "epsilon_callback = EpsilonCallback(adaptive_eps, reward_history, verbose=1)\n",
    "\n",
    "total_timesteps = 20_000_000\n",
    "progress_bar_callback = TQDMProgressCallback(total_timesteps=total_timesteps, inital=model.num_timesteps)\n",
    "ml_callback = MLflowCallback(\n",
    "    best_model_path=\"models/custom_DeepMind_continue_exploration.zip\",\n",
    "    experiment_name=\"DQN_SpaceInvaders\",\n",
    "    run_name=\"DQN_Run\",\n",
    "    log_freq=50_000\n",
    ")\n",
    "\n",
    "experiment_name = \"DQN_SpaceInvaders\"\n",
    "exist_experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "if not exist_experiment:\n",
    "    mlflow.create_experiment(experiment_name)\n",
    "mlflow.set_experiment(experiment_name)\n",
    "with mlflow.start_run(run_name=\"DQN_Run_custom_deepMind_continue\"):\n",
    "    mlflow.log_param(\"env_name\", env_name)\n",
    "    mlflow.log_param(\"total_timesteps\", total_timesteps)\n",
    "    mlflow.log_param(\"learning_rate\", 25e-5)\n",
    "    mlflow.log_param(\"buffer_size\", 400_000)\n",
    "    mlflow.log_param(\"exploration_fraction\", 0.1)\n",
    "    mlflow.log_param(\"exploration_final_eps\", 0.05)\n",
    "    mlflow.log_param(\"exploration_initial_eps\", 1.0)\n",
    "    mlflow.log_param(\"batch_size\", 32)\n",
    "    mlflow.log_param(\"gamma\", 0.95)\n",
    "    mlflow.log_param(\"learning_starts\", 10000)\n",
    "    mlflow.log_param(\"target_update_interval\", 10000)\n",
    "    mlflow.log_param(\"file_name\", \"deep_mind_continue_exploration.zip\")\n",
    "\n",
    "    mlflow.log_params(vars(adaptive_eps))\n",
    "\n",
    "    model = DQN(\"CnnPolicy\", env, verbose=0,\n",
    "                policy_kwargs=policy_kwargs,\n",
    "                learning_rate=25e-5, buffer_size=400_000,\n",
    "                exploration_fraction=0.1, exploration_initial_eps=0.3,\n",
    "                exploration_final_eps=0.05, target_update_interval=10000,\n",
    "                batch_size=32, learning_starts=10_000, gamma=0.95,seed=23)\n",
    "    model.policy.load_state_dict(model_old.policy.state_dict())\n",
    "\n",
    "    t_model = model.learn(total_timesteps=total_timesteps, callback=[progress_bar_callback, ml_callback, TestCallBack(env, test_timesteps=400_000), epsilon_callback], log_interval=2_000)\n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Training Progress:   0%|          | 5458/20000000 [00:00<?, ?step/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4be3934229dc433e86204030e554b8db"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[12], line 51\u001B[0m\n\u001B[1;32m     43\u001B[0m model \u001B[38;5;241m=\u001B[39m DQN(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCnnPolicy\u001B[39m\u001B[38;5;124m\"\u001B[39m, env, verbose\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m,\n\u001B[1;32m     44\u001B[0m             policy_kwargs\u001B[38;5;241m=\u001B[39mpolicy_kwargs,\n\u001B[1;32m     45\u001B[0m             learning_rate\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m25e-5\u001B[39m, buffer_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m400_000\u001B[39m,\n\u001B[1;32m     46\u001B[0m             exploration_fraction\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.1\u001B[39m, exploration_initial_eps\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.3\u001B[39m,\n\u001B[1;32m     47\u001B[0m             exploration_final_eps\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.05\u001B[39m, target_update_interval\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10000\u001B[39m,\n\u001B[1;32m     48\u001B[0m             batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m32\u001B[39m, learning_starts\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10_000\u001B[39m, gamma\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.95\u001B[39m,seed\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m23\u001B[39m)\n\u001B[1;32m     49\u001B[0m model\u001B[38;5;241m.\u001B[39mpolicy\u001B[38;5;241m.\u001B[39mload_state_dict(model_old\u001B[38;5;241m.\u001B[39mpolicy\u001B[38;5;241m.\u001B[39mstate_dict())\n\u001B[0;32m---> 51\u001B[0m t_model \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mlearn(total_timesteps\u001B[38;5;241m=\u001B[39mtotal_timesteps, callback\u001B[38;5;241m=\u001B[39m[progress_bar_callback, ml_callback, TestCallBack(env, test_timesteps\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m400_000\u001B[39m), epsilon_callback], log_interval\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2_000\u001B[39m, reset_num_timesteps\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "File \u001B[0;32m~/anaconda3/envs/pytorch/lib/python3.12/site-packages/stable_baselines3/dqn/dqn.py:267\u001B[0m, in \u001B[0;36mDQN.learn\u001B[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001B[0m\n\u001B[1;32m    258\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mlearn\u001B[39m(\n\u001B[1;32m    259\u001B[0m     \u001B[38;5;28mself\u001B[39m: SelfDQN,\n\u001B[1;32m    260\u001B[0m     total_timesteps: \u001B[38;5;28mint\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    265\u001B[0m     progress_bar: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m    266\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m SelfDQN:\n\u001B[0;32m--> 267\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39mlearn(\n\u001B[1;32m    268\u001B[0m         total_timesteps\u001B[38;5;241m=\u001B[39mtotal_timesteps,\n\u001B[1;32m    269\u001B[0m         callback\u001B[38;5;241m=\u001B[39mcallback,\n\u001B[1;32m    270\u001B[0m         log_interval\u001B[38;5;241m=\u001B[39mlog_interval,\n\u001B[1;32m    271\u001B[0m         tb_log_name\u001B[38;5;241m=\u001B[39mtb_log_name,\n\u001B[1;32m    272\u001B[0m         reset_num_timesteps\u001B[38;5;241m=\u001B[39mreset_num_timesteps,\n\u001B[1;32m    273\u001B[0m         progress_bar\u001B[38;5;241m=\u001B[39mprogress_bar,\n\u001B[1;32m    274\u001B[0m     )\n",
      "File \u001B[0;32m~/anaconda3/envs/pytorch/lib/python3.12/site-packages/stable_baselines3/common/off_policy_algorithm.py:347\u001B[0m, in \u001B[0;36mOffPolicyAlgorithm.learn\u001B[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001B[0m\n\u001B[1;32m    345\u001B[0m         \u001B[38;5;66;03m# Special case when the user passes `gradient_steps=0`\u001B[39;00m\n\u001B[1;32m    346\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m gradient_steps \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m--> 347\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrain(batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbatch_size, gradient_steps\u001B[38;5;241m=\u001B[39mgradient_steps)\n\u001B[1;32m    349\u001B[0m callback\u001B[38;5;241m.\u001B[39mon_training_end()\n\u001B[1;32m    351\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\n",
      "File \u001B[0;32m~/anaconda3/envs/pytorch/lib/python3.12/site-packages/stable_baselines3/dqn/dqn.py:217\u001B[0m, in \u001B[0;36mDQN.train\u001B[0;34m(self, gradient_steps, batch_size)\u001B[0m\n\u001B[1;32m    215\u001B[0m \u001B[38;5;66;03m# Optimize the policy\u001B[39;00m\n\u001B[1;32m    216\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpolicy\u001B[38;5;241m.\u001B[39moptimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m--> 217\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[1;32m    218\u001B[0m \u001B[38;5;66;03m# Clip gradient norm\u001B[39;00m\n\u001B[1;32m    219\u001B[0m th\u001B[38;5;241m.\u001B[39mnn\u001B[38;5;241m.\u001B[39mutils\u001B[38;5;241m.\u001B[39mclip_grad_norm_(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpolicy\u001B[38;5;241m.\u001B[39mparameters(), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmax_grad_norm)\n",
      "File \u001B[0;32m~/anaconda3/envs/pytorch/lib/python3.12/site-packages/torch/_tensor.py:521\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    511\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    512\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    513\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    514\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    519\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[1;32m    520\u001B[0m     )\n\u001B[0;32m--> 521\u001B[0m torch\u001B[38;5;241m.\u001B[39mautograd\u001B[38;5;241m.\u001B[39mbackward(\n\u001B[1;32m    522\u001B[0m     \u001B[38;5;28mself\u001B[39m, gradient, retain_graph, create_graph, inputs\u001B[38;5;241m=\u001B[39minputs\n\u001B[1;32m    523\u001B[0m )\n",
      "File \u001B[0;32m~/anaconda3/envs/pytorch/lib/python3.12/site-packages/torch/autograd/__init__.py:289\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    284\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m    286\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[1;32m    287\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    288\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 289\u001B[0m _engine_run_backward(\n\u001B[1;32m    290\u001B[0m     tensors,\n\u001B[1;32m    291\u001B[0m     grad_tensors_,\n\u001B[1;32m    292\u001B[0m     retain_graph,\n\u001B[1;32m    293\u001B[0m     create_graph,\n\u001B[1;32m    294\u001B[0m     inputs,\n\u001B[1;32m    295\u001B[0m     allow_unreachable\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m    296\u001B[0m     accumulate_grad\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m    297\u001B[0m )\n",
      "File \u001B[0;32m~/anaconda3/envs/pytorch/lib/python3.12/site-packages/torch/autograd/graph.py:768\u001B[0m, in \u001B[0;36m_engine_run_backward\u001B[0;34m(t_outputs, *args, **kwargs)\u001B[0m\n\u001B[1;32m    766\u001B[0m     unregister_hooks \u001B[38;5;241m=\u001B[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[1;32m    767\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 768\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m Variable\u001B[38;5;241m.\u001B[39m_execution_engine\u001B[38;5;241m.\u001B[39mrun_backward(  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[1;32m    769\u001B[0m         t_outputs, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[1;32m    770\u001B[0m     )  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[1;32m    771\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    772\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "OHYryKd1Gb2b"
   },
   "source": [
    "# Testing part to calculate the mean reward\n",
    "weights_filename = 'dqn_{}_weights.h5f'.format(env_name)\n",
    "dqn.load_weights(weights_filename)\n",
    "dqn.test(env, nb_episodes=10, visualize=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-NAlu8b1Gb2b"
   },
   "source": [
    "3. Justificación de los parámetros seleccionados y de los resultados obtenidos"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "---"
   ],
   "metadata": {
    "id": "ANFQiicXK3sO"
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch] *",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
