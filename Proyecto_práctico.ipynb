{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Actividad - Proyecto práctico\n",
    "\n",
    "\n",
    "> La actividad se desarrollará en grupos pre-definidos de 2-3 alumnos. Se debe indicar los nombres en orden alfabético (de apellidos). Recordad que esta actividad se corresponde con un 30% de la nota final de la asignatura. Se debe entregar entregar el trabajo en la presente notebook.\n",
    "*   Alumno 1:\n",
    "*   Alumno 2:\n",
    "*   Alumno 3:\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "jUehXgCyIRdq"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## **PARTE 1** - Instalación y requisitos previos\n",
    "\n",
    "> Las prácticas han sido preparadas para poder realizarse en el entorno de trabajo de Google Colab. Sin embargo, esta plataforma presenta ciertas incompatibilidades a la hora de visualizar la renderización en gym. Por ello, para obtener estas visualizaciones, se deberá trasladar el entorno de trabajo a local. Por ello, el presente dosier presenta instrucciones para poder trabajar en ambos entornos. Siga los siguientes pasos para un correcto funcionamiento:\n",
    "1.   **LOCAL:** Preparar el enviroment, siguiendo las intrucciones detalladas en la sección *1.1.Preparar enviroment*.\n",
    "2.  **AMBOS:** Modificar las variables \"mount\" y \"drive_mount\" a la carpeta de trabajo en drive en el caso de estar en Colab, y ejecturar la celda *1.2.Localizar entorno de trabajo*.\n",
    "3. **COLAB:** se deberá ejecutar las celdas correspondientes al montaje de la carpeta de trabajo en Drive. Esta corresponde a la sección *1.3.Montar carpeta de datos local*.\n",
    "4.  **AMBOS:** Instalar las librerías necesarias, siguiendo la sección *1.4.Instalar librerías necesarias*.\n"
   ],
   "metadata": {
    "id": "JwpYlnjWJhS9"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "### 1.1. Preparar enviroment (solo local)\n",
    "\n",
    "\n",
    "\n",
    "> Para preparar el entorno de trabajo en local, se han seguido los siguientes pasos:\n",
    "1. En Windows, puede ser necesario instalar las C++ Build Tools. Para ello, siga los siguientes pasos: https://towardsdatascience.com/how-to-install-openai-gym-in-a-windows-environment-338969e24d30.\n",
    "2. Instalar Anaconda\n",
    "3. Siguiendo el código que se presenta comentado en la próxima celda: Crear un enviroment, cambiar la ruta de trabajo, e instalar librerías básicas.\n",
    "\n",
    "\n",
    "```\n",
    "conda create --name miar_rl python=3.8\n",
    "conda activate miar_rl\n",
    "cd \"PATH_TO_FOLDER\"\n",
    "conda install git\n",
    "pip install jupyter\n",
    "```\n",
    "\n",
    "\n",
    "4. Abrir la notebook con *jupyter-notebook*.\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "jupyter-notebook\n",
    "```\n"
   ],
   "metadata": {
    "id": "RU2BPrK2JkP0"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "### 1.2. Localizar entorno de trabajo: Google colab o local"
   ],
   "metadata": {
    "id": "w-kixNPiJqTc"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from networkx.tests.test_all_random_functions import progress\n",
    "from sympy import print_rcode\n",
    "\n",
    "# ATENCIÓN!! Modificar ruta relativa a la práctica si es distinta (drive_root)\n",
    "mount='/content/gdrive'\n",
    "drive_root = mount + \"/My Drive/08_MIAR/actividades/proyecto practico\"\n",
    "\n",
    "try:\n",
    "  from google.colab import drive\n",
    "  IN_COLAB=True\n",
    "except:\n",
    "  IN_COLAB=False"
   ],
   "metadata": {
    "id": "S_YDFwZ-JscI"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "### 1.3. Montar carpeta de datos local (solo Colab)"
   ],
   "metadata": {
    "id": "8Dp_a1iBJ0tf"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Switch to the directory on the Google Drive that you want to use\n",
    "import os\n",
    "if IN_COLAB:\n",
    "  print(\"We're running Colab\")\n",
    "\n",
    "  if IN_COLAB:\n",
    "    # Mount the Google Drive at mount\n",
    "    print(\"Colab: mounting Google drive on \", mount)\n",
    "\n",
    "    drive.mount(mount)\n",
    "\n",
    "    # Create drive_root if it doesn't exist\n",
    "    create_drive_root = True\n",
    "    if create_drive_root:\n",
    "      print(\"\\nColab: making sure \", drive_root, \" exists.\")\n",
    "      os.makedirs(drive_root, exist_ok=True)\n",
    "\n",
    "    # Change to the directory\n",
    "    print(\"\\nColab: Changing directory to \", drive_root)\n",
    "    %cd $drive_root\n",
    "# Verify we're in the correct working directory\n",
    "%pwd\n",
    "print(\"Archivos en el directorio: \")\n",
    "print(os.listdir())"
   ],
   "metadata": {
    "id": "I6n7MIefJ21i"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "### 1.4. Instalar librerías necesarias"
   ],
   "metadata": {
    "id": "i1ZSL5bpJ560"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "if IN_COLAB:\n",
    "  %pip install gym==0.17.3\n",
    "  %pip install git+https://github.com/Kojoley/atari-py.git\n",
    "  %pip install keras-rl2==1.0.5\n",
    "  %pip install tensorflow==2.8\n",
    "else:\n",
    "  %pip install gym==0.17.3\n",
    "  %pip install git+https://github.com/Kojoley/atari-py.git\n",
    "  %pip install pyglet==1.5.0\n",
    "  %pip install h5py==3.1.0\n",
    "  %pip install Pillow==9.5.0\n",
    "  %pip install keras-rl2==1.0.5\n",
    "  %pip install Keras==2.2.4\n",
    "  %pip install tensorflow==2.5.3\n",
    "  %pip install torch==2.0.1\n",
    "  %pip install agents==1.4.0"
   ],
   "metadata": {
    "id": "UbVRjvHCJ8UF"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4hzP_5ZuGb2X"
   },
   "source": [
    "---\n",
    "## **PARTE 2**. Enunciado\n",
    "\n",
    "Consideraciones a tener en cuenta:\n",
    "\n",
    "- El entorno sobre el que trabajaremos será _SpaceInvaders-v0_ y el algoritmo que usaremos será _DQN_.\n",
    "\n",
    "- Para nuestro ejercicio, el requisito mínimo será alcanzado cuando el agente consiga una **media de recompensa por encima de 20 puntos en modo test**. Por ello, esta media de la recompensa se calculará a partir del código de test en la última celda del notebook.\n",
    "\n",
    "Este proyecto práctico consta de tres partes:\n",
    "\n",
    "1.   Implementar la red neuronal que se usará en la solución\n",
    "2.   Implementar las distintas piezas de la solución DQN\n",
    "3.   Justificar la respuesta en relación a los resultados obtenidos\n",
    "\n",
    "**Rúbrica**: Se valorará la originalidad en la solución aportada, así como la capacidad de discutir los resultados de forma detallada. El requisito mínimo servirá para aprobar la actividad, bajo premisa de que la discusión del resultado sera apropiada.\n",
    "\n",
    "IMPORTANTE:\n",
    "\n",
    "* Si no se consigue una puntuación óptima, responder sobre la mejor puntuación obtenida.\n",
    "* Para entrenamientos largos, recordad que podéis usar checkpoints de vuestros modelos para retomar los entrenamientos. En este caso, recordad cambiar los parámetros adecuadamente (sobre todo los relacionados con el proceso de exploración).\n",
    "* Se deberá entregar unicamente el notebook y los pesos del mejor modelo en un fichero .zip, de forma organizada.\n",
    "* Cada alumno deberá de subir la solución de forma individual."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## **PARTE 3**. Desarrollo y preguntas"
   ],
   "metadata": {
    "id": "6_b3mzw8IzJP"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "duPmUNOVGb2a"
   },
   "source": [
    "#### Importar librerías"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "j3eRhgI-Gb2a",
    "ExecuteTime": {
     "end_time": "2025-06-21T17:55:10.794025Z",
     "start_time": "2025-06-21T17:55:10.784012Z"
    }
   },
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import mlflow\n",
    "from stable_baselines3 import DQN, PPO, A2C\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import os\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecFrameStack, SubprocVecEnv\n",
    "from stable_baselines3.common.callbacks import ProgressBarCallback, BaseCallback\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "import gymnasium as gym\n",
    "import ale_py\n",
    "\n",
    "\n",
    "from torchvision.models import mobilenet_v2\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import mlflow\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "from stable_baselines3.common.atari_wrappers import AtariWrapper\n",
    "from gymnasium.wrappers import AtariPreprocessing\n",
    "\n",
    "from sb3_contrib import QRDQN"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f4jgQjzoGb2a"
   },
   "source": [
    "\n",
    "#### Configuración base"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-21T21:02:26.311663Z",
     "start_time": "2025-06-21T21:02:26.205640Z"
    }
   },
   "cell_type": "code",
   "source": [
    "INPUT_SHAPE = (84, 84)\n",
    "WINDOW_LENGTH = 4\n",
    "gym.register_envs(ale_py)\n",
    "class CustomPenaltyWrapper(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.last_action = None\n",
    "        self.same_action_count = 0\n",
    "        self.last_lives = None\n",
    "        self.no_shoot_count = 0\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        obs, info = self.env.reset(**kwargs)\n",
    "        self.last_action = None\n",
    "        self.same_action_count = 0\n",
    "        self.no_shoot_count = 0\n",
    "\n",
    "        if \"lives\" in info:\n",
    "            self.last_lives = info[\"lives\"]\n",
    "        else:\n",
    "            self.last_lives = 3\n",
    "\n",
    "        return obs, info\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "\n",
    "        # --- Penalty for repeated no-op\n",
    "        if action == 0:\n",
    "            if self.last_action == 0:\n",
    "                self.same_action_count += 1\n",
    "            else:\n",
    "                self.same_action_count = 1\n",
    "        else:\n",
    "            self.same_action_count = 0\n",
    "        self.last_action = action\n",
    "\n",
    "        if self.same_action_count >= 3:\n",
    "            reward -= 1.0\n",
    "\n",
    "        # --- Penalty for getting hit\n",
    "        if \"lives\" in info and self.last_lives is not None:\n",
    "            if info[\"lives\"] < self.last_lives:\n",
    "                reward -= 0.5\n",
    "            self.last_lives = info[\"lives\"]\n",
    "\n",
    "        # --- Penalty for not shooting for too long\n",
    "        if int(action) in [1, 4, 5]:  # shooting actions\n",
    "            self.no_shoot_count = 0\n",
    "        else:\n",
    "            self.no_shoot_count += 1\n",
    "            if self.no_shoot_count >= 10:\n",
    "                reward -= 0.7\n",
    "\n",
    "        reward = np.clip(reward, -1.0, 1.0)\n",
    "\n",
    "        return obs, reward, terminated, truncated, info\n",
    "\n",
    "\n",
    "class NormalizeInput(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=0.0, high=1.0,\n",
    "            shape=self.observation_space.shape,\n",
    "            dtype=np.float32\n",
    "        )\n",
    "\n",
    "    def observation(self, observation):\n",
    "        return observation.astype(np.float32) / 255.0\n",
    "\n",
    "env_name = 'SpaceInvadersNoFrameskip-v4'\n",
    "env = gym.make(env_name)\n",
    "#En este caso el AtariWrapper hace lo mismo que Clipreward además de añadir el preprocesado de las imágenes\n",
    "#Normalizamos las imágenes a 0-1\n",
    "# env = NormalizeInput(env)\n",
    "env = AtariWrapper(env, frame_skip=4)\n",
    "obs, _ = env.reset()\n",
    "print(\"Observación inicial:\", obs.shape)\n",
    "\n",
    "normal_env = NormalizeInput(env)\n",
    "pen_env = CustomPenaltyWrapper(normal_env)  # Añadimos el wrapper de penalización\n",
    "\n",
    "# parallel_env = SubprocVecEnv([normal_env for _ in range(8)])\n",
    "\n",
    "#Train env con penalizaciones\n",
    "env = Monitor(pen_env)\n",
    "env = DummyVecEnv([lambda: env])  # Convertimos a un entorno vectorizado\n",
    "# # # Se crea el entorno de vectores y se apilan los frames\n",
    "env = VecFrameStack(env, 4)\n",
    "\n",
    "#Test env sin penalizaciones\n",
    "normal_env = Monitor(normal_env)\n",
    "normal_env = DummyVecEnv([lambda: normal_env])  # Convertimos a un entorno vectorizado\n",
    "# # # Se crea el entorno de vectores y se apilan los frames\n",
    "normal_env = VecFrameStack(normal_env, 4)\n",
    "\n",
    "np.random.seed(123)\n",
    "obs = env.reset()\n",
    "nb_actions = env.action_space.n\n",
    "# print(env.shape)\n",
    "print(obs.shape)\n",
    "print(nb_actions)\n",
    "print(\"maximo de altura\", max(obs[0, :, 0, :].flatten()))\n",
    "print(\"maximo de ancho\", max(obs[0, 0 :, :].flatten()))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observación inicial: (84, 84, 1)\n",
      "(1, 84, 84, 4)\n",
      "6\n",
      "maximo de altura 0.30980393\n",
      "maximo de ancho 0.52156866\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-20T18:14:16.127885Z",
     "start_time": "2025-06-20T18:14:16.117871Z"
    }
   },
   "cell_type": "code",
   "source": "obs, info = pen_env.reset()",
   "outputs": [],
   "execution_count": 81
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-20T17:56:01.762764Z",
     "start_time": "2025-06-20T17:56:01.655155Z"
    }
   },
   "cell_type": "code",
   "source": [
    "obs, info = pen_env.reset()\n",
    "done = False\n",
    "total_reward = 0\n",
    "\n",
    "while not done:\n",
    "    action = 0\n",
    "    obs, reward, terminated, truncated, info = pen_env.step(action)\n",
    "    total_reward += reward\n",
    "    if truncated or terminated:\n",
    "        done = True\n",
    "\n",
    "    print(\"Reward:\", reward)\n",
    "print(f\"Total reward: {total_reward}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward: 0.0\n",
      "Reward: 0.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Reward: -1.0\n",
      "Total reward: -159.0\n"
     ]
    }
   ],
   "execution_count": 67
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7yitXTADGb2b"
   },
   "source": [
    "1. Implementación de la red neuronal"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "O4GKrfWSGb2b",
    "ExecuteTime": {
     "end_time": "2025-06-18T13:22:07.771854Z",
     "start_time": "2025-06-18T13:22:07.768096Z"
    }
   },
   "source": [
    "class MobileNetFeatureExtractor(BaseFeaturesExtractor):\n",
    "    def __init__(self, observation_space, features_dim=256):\n",
    "        super().__init__(observation_space, features_dim)\n",
    "\n",
    "        # Pretrained MobileNetV2 without the classifier\n",
    "        weights = torchvision.models.MobileNet_V2_Weights.DEFAULT\n",
    "        self.backbone = torchvision.models.mobilenet_v2(weights=weights)\n",
    "\n",
    "        # Freeze weights (optional)\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Compute shape by doing one forward pass\n",
    "        with torch.no_grad():\n",
    "            sample = torch.as_tensor(observation_space.sample()[None]).float()\n",
    "            if sample.shape[1] != 3:  # Convert grayscale to 3 channels\n",
    "                sample = sample.repeat(1, 3, 1, 1)\n",
    "            n_flatten = self.backbone(sample).view(sample.shape[0], -1).shape[1]\n",
    "\n",
    "        self.projector = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(n_flatten, n_flatten // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(n_flatten // 2, n_flatten // 3),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(n_flatten // 3, features_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, obs):\n",
    "        # Convert 1-channel grayscale to 3 channels if needed\n",
    "        if obs.shape[1] == 1:\n",
    "            obs = obs.repeat(1, 3, 1, 1)\n",
    "        features = self.backbone(obs)\n",
    "        return self.projector(features)"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Convirtiendo las imágenes a 3 canales"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T05:12:33.897196Z",
     "start_time": "2025-06-15T05:12:33.893487Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class VitB16FeaturesExtractor(BaseFeaturesExtractor):\n",
    "    def __init__(self, observation_space, features_dim=256):\n",
    "        super().__init__(observation_space, features_dim)\n",
    "\n",
    "        # Load a pretrained ViT model\n",
    "        weights = torchvision.models.ViT_B_16_Weights.DEFAULT\n",
    "        self.backbone = torchvision.models.vit_b_16(weights=weights).cuda()\n",
    "\n",
    "        # Freeze weights (optional)\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Compute shape by doing one forward pass\n",
    "        with torch.no_grad():\n",
    "            sample = torch.as_tensor(observation_space.sample()[None]).float()\n",
    "            if sample.shape[1] != 3:  # Convert grayscale to 3 channels\n",
    "                sample = sample.repeat(1, 3, 1, 1)\n",
    "            sample = self._preprocess(sample).cuda()\n",
    "            n_flatten = self.backbone(sample).view(sample.shape[0], -1).shape[1]\n",
    "\n",
    "        self.projector = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(n_flatten, features_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def _preprocess(self, observation):\n",
    "        # Preprocess the observation to match the input requirements of ViT\n",
    "        batch_resize = F.interpolate(\n",
    "            observation, size=(224, 224), mode='bilinear', align_corners=False\n",
    "        )\n",
    "        return batch_resize\n",
    "\n",
    "\n",
    "    def forward(self, obs):\n",
    "        # Convert 1-channel grayscale to 3 channels if needed\n",
    "        obs = self._preprocess(obs).cuda()\n",
    "        if obs.shape[1] == 1:\n",
    "            obs = obs.repeat(1, 3, 1, 1)\n",
    "        features = self.backbone(obs)\n",
    "        return self.projector(features)"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Escala de grises"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T04:16:00.500380Z",
     "start_time": "2025-06-18T04:16:00.496739Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class VitB16FeaturesExtractor(BaseFeaturesExtractor):\n",
    "    def __init__(self, observation_space, features_dim=256):\n",
    "        super().__init__(observation_space, features_dim)\n",
    "\n",
    "        self.chanel_mapper = nn.Conv2d(\n",
    "            in_channels=1, out_channels=3, kernel_size=1, stride=1, padding=0\n",
    "        )\n",
    "        self.uppscaler = nn.Upsample(size=(224, 224), mode='bilinear', align_corners=False)\n",
    "        # Load a pretrained ViT model\n",
    "        weights = torchvision.models.ViT_B_16_Weights.DEFAULT\n",
    "        self.backbone = torchvision.models.vit_b_16(weights=weights).cuda()\n",
    "\n",
    "        # Freeze weights (optional)\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Compute shape by doing one forward pass\n",
    "        with torch.no_grad():\n",
    "            sample = torch.as_tensor(observation_space.sample()[None]).float()\n",
    "            if sample.shape[1] != 3:  # Convert grayscale to 3 channels\n",
    "                sample = sample.repeat(1, 3, 1, 1)\n",
    "            sample = self._preprocess(sample).cuda()\n",
    "            n_flatten = self.backbone(sample).view(sample.shape[0], -1).shape[1]\n",
    "\n",
    "        self.projector = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(n_flatten, features_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def _preprocess(self, observation):\n",
    "        # Preprocess the observation to match the input requirements of ViT\n",
    "        batch_resize = F.interpolate(\n",
    "            observation, size=(224, 224), mode='bilinear', align_corners=False\n",
    "        )\n",
    "        return batch_resize\n",
    "\n",
    "\n",
    "    def forward(self, obs):\n",
    "        # Convert 1-channel grayscale to 3 channels if needed\n",
    "        # obs = self._preprocess(obs).cuda()\n",
    "        obs = self.chanel_mapper(obs).cuda()\n",
    "        obs = self.uppscaler(obs).cuda()\n",
    "        # if obs.shape[1] == 1:\n",
    "        #     obs = obs.repeat(1, 3, 1, 1)\n",
    "        features = self.backbone(obs)\n",
    "        return self.projector(features)\n"
   ],
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T20:53:23.152279Z",
     "start_time": "2025-06-15T20:53:23.148792Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ResNet152FeaturesExtractor(BaseFeaturesExtractor):\n",
    "    def __init__(self, observation_space, features_dim=256):\n",
    "        super().__init__(observation_space, features_dim)\n",
    "\n",
    "        # Load a pretrained ResNet152 model\n",
    "        weights = torchvision.models.ResNet152_Weights.DEFAULT\n",
    "        self.backbone = torchvision.models.resnet152(weights=weights)\n",
    "\n",
    "        # Freeze weights (optional)\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Compute shape by doing one forward pass\n",
    "        with torch.no_grad():\n",
    "            sample = torch.as_tensor(observation_space.sample()[None]).float()\n",
    "            if sample.shape[1] != 3:  # Convert grayscale to 3 channels\n",
    "                sample = sample.repeat(1, 3, 1, 1)\n",
    "            n_flatten = self.backbone(sample).view(sample.shape[0], -1).shape[1]\n",
    "\n",
    "        self.projector = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(n_flatten, features_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, obs):\n",
    "        # Convert 1-channel grayscale to 3 channels if needed\n",
    "        if obs.shape[1] == 1:\n",
    "            obs = obs.repeat(1, 3, 1, 1)\n",
    "        features = self.backbone(obs)\n",
    "        return self.projector(features)"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### CNNConnectedDeep\n",
    "\n",
    "- Prueba inicial con reescalado y en escala de grises."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-16T00:18:51.272846Z",
     "start_time": "2025-06-16T00:18:51.265964Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class CNNConnectedDeep(BaseFeaturesExtractor):\n",
    "    def __init__(self, observation_space, features_dim=256):\n",
    "        super().__init__(observation_space, features_dim)\n",
    "\n",
    "        # Primeras capas\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(32, 32, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.dropout = nn.Dropout(0.4)\n",
    "\n",
    "        # Concatenación de conv1 y conv3\n",
    "        # 32 (resized conv1) + 64 = 96\n",
    "        # Bloques de compresión adicionales con conexiones\n",
    "        self.conv4 = nn.Conv2d(96, 128, kernel_size=3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(128)\n",
    "\n",
    "        self.conv5 = nn.Conv2d(128, 160, kernel_size=3, padding=1)\n",
    "        self.bn5 = nn.BatchNorm2d(160)\n",
    "\n",
    "        self.conv6 = nn.Conv2d(160, 192, kernel_size=3, padding=1)\n",
    "        self.bn6 = nn.BatchNorm2d(192)\n",
    "\n",
    "        self.conv7 = nn.Conv2d(192 + 128, 224, kernel_size=3, padding=1)  # concat con out4\n",
    "        self.bn7 = nn.BatchNorm2d(224)\n",
    "\n",
    "        self.conv8 = nn.Conv2d(224 + 160, 256, kernel_size=3, padding=1)  # concat con out5\n",
    "        self.bn8 = nn.BatchNorm2d(256)\n",
    "\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d((4, 4))  # reduce a [B, 256, 4, 4]\n",
    "\n",
    "        # Flatten: 256 * 4 * 4 = 4096 → muy alto → reducimos\n",
    "        self.fc1 = nn.Linear(256 * 4 * 4, 512)\n",
    "        self.fc2 = nn.Linear(512, features_dim)\n",
    "\n",
    "    def _preprocess(self, observation):\n",
    "        r, g, b = observation[:, 0:1, :, :], observation[:, 1:2, :, :], observation[:, 2:3, :, :]\n",
    "        gray = 0.2989 * r + 0.5870 * g + 0.1140 * b\n",
    "        gray3 = gray.repeat(1, 3, 1, 1)  # Convert to 3 channels\n",
    "\n",
    "        batch_resize = F.interpolate(\n",
    "            gray3, size=(224, 224), mode='bilinear', align_corners=False\n",
    "        )\n",
    "        return batch_resize\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self._preprocess(x)  # Convert grayscale to 3 channels and resize to 224x224\n",
    "        out1 = F.relu(self.bn1(self.conv1(x)))  # [B, 32, 224, 224]\n",
    "        out2 = F.relu(self.bn2(self.conv2(out1)))\n",
    "        out2 = out1 + out2  # Residual connection\n",
    "        out2 = self.pool(out2)  # [B, 32, 112, 112]\n",
    "\n",
    "        out3 = F.relu(self.bn3(self.conv3(out2)))\n",
    "        out3 = self.pool(out3)  # [B, 64, 56, 56]\n",
    "\n",
    "        out1_resized = F.interpolate(out1, size=out3.shape[2:])\n",
    "        concat1 = torch.cat((out3, out1_resized), dim=1)  # [B, 96, 56, 56]\n",
    "\n",
    "        # Bloque 4\n",
    "        out4 = F.relu(self.bn4(self.conv4(concat1)))\n",
    "        out4 = self.pool(out4)  # [B, 128, 28, 28]\n",
    "\n",
    "        # Bloque 5\n",
    "        out5 = F.relu(self.bn5(self.conv5(out4)))\n",
    "        out5 = self.pool(out5)  # [B, 160, 14, 14]\n",
    "\n",
    "        # Bloque 6\n",
    "        out6 = F.relu(self.bn6(self.conv6(out5)))\n",
    "        out6 = self.pool(out6)  # [B, 192, 7, 7]\n",
    "\n",
    "        # Concat out4 (resized) con out6\n",
    "        out4_resized = F.interpolate(out4, size=out6.shape[2:])\n",
    "        concat2 = torch.cat((out6, out4_resized), dim=1)  # [B, 192+128=320, 7, 7]\n",
    "        out7 = F.relu(self.bn7(self.conv7(concat2)))\n",
    "\n",
    "        # Concat out5 (resized) con out7\n",
    "        out5_resized = F.interpolate(out5, size=out7.shape[2:])\n",
    "        concat3 = torch.cat((out7, out5_resized), dim=1)  # [B, 224+160=384, 7, 7]\n",
    "        out8 = F.relu(self.bn8(self.conv8(concat3)))\n",
    "\n",
    "        x = self.global_pool(out8)  # [B, 256, 4, 4]\n",
    "        x = x.view(x.size(0), -1)   # Flatten → [B, 4096]\n",
    "\n",
    "        x = self.dropout(F.relu(self.fc1(x)))  # 4096 → 512\n",
    "        out = self.fc2(x)  # 512 → num_classes\n",
    "\n",
    "        return out\n"
   ],
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- Adaptación a nuevo tamaño de red 84,84"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T02:38:15.601826Z",
     "start_time": "2025-06-18T02:38:15.596482Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class CNNConnectedDeep(BaseFeaturesExtractor):\n",
    "    def __init__(self, observation_space, features_dim=256):\n",
    "        super().__init__(observation_space, features_dim)\n",
    "\n",
    "        # Primeras capas\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(32, 32, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.dropout = nn.Dropout(0.4)\n",
    "\n",
    "        # Concatenación de conv1 y conv3\n",
    "        # 32 (resized conv1) + 64 = 96\n",
    "        # Bloques de compresión adicionales con conexiones\n",
    "        self.conv4 = nn.Conv2d(96, 128, kernel_size=3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(128)\n",
    "\n",
    "        self.conv5 = nn.Conv2d(128, 160, kernel_size=3, padding=1)\n",
    "        self.bn5 = nn.BatchNorm2d(160)\n",
    "\n",
    "        self.conv6 = nn.Conv2d(160, 192, kernel_size=3, padding=1)\n",
    "        self.bn6 = nn.BatchNorm2d(192)\n",
    "\n",
    "        self.conv7 = nn.Conv2d(192 + 128, 224, kernel_size=3, padding=1)  # concat con out4\n",
    "        self.bn7 = nn.BatchNorm2d(224)\n",
    "\n",
    "        self.conv8 = nn.Conv2d(224 + 160, 256, kernel_size=3, padding=1)  # concat con out5\n",
    "        self.bn8 = nn.BatchNorm2d(256)\n",
    "\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d((4, 4))  # reduce a [B, 256, 4, 4]\n",
    "\n",
    "        # Flatten: 256 * 4 * 4 = 4096 → muy alto → reducimos\n",
    "        self.fc1 = nn.Linear(256 * 4 * 4, 512)\n",
    "        self.fc2 = nn.Linear(512, features_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out1 = F.relu(self.bn1(self.conv1(x)))  # [B, 32, 224, 224]\n",
    "        out2 = F.relu(self.bn2(self.conv2(out1)))\n",
    "        out2 = out1 + out2  # Residual connection\n",
    "        out2 = self.pool(out2)  # [B, 32, 112, 112]\n",
    "\n",
    "        out3 = F.relu(self.bn3(self.conv3(out2)))\n",
    "        out3 = self.pool(out3)  # [B, 64, 56, 56]\n",
    "\n",
    "        out1_resized = F.interpolate(out1, size=out3.shape[2:])\n",
    "        concat1 = torch.cat((out3, out1_resized), dim=1)  # [B, 96, 56, 56]\n",
    "\n",
    "        # Bloque 4\n",
    "        out4 = F.relu(self.bn4(self.conv4(concat1)))\n",
    "        out4 = self.pool(out4)  # [B, 128, 28, 28]\n",
    "\n",
    "        # Bloque 5\n",
    "        out5 = F.relu(self.bn5(self.conv5(out4)))\n",
    "        out5 = self.pool(out5)  # [B, 160, 14, 14]\n",
    "\n",
    "        # Bloque 6\n",
    "        out6 = F.relu(self.bn6(self.conv6(out5)))\n",
    "        out6 = self.pool(out6)  # [B, 192, 7, 7]\n",
    "\n",
    "        # Concat out4 (resized) con out6\n",
    "        out4_resized = F.interpolate(out4, size=out6.shape[2:])\n",
    "        concat2 = torch.cat((out6, out4_resized), dim=1)  # [B, 192+128=320, 7, 7]\n",
    "        out7 = F.relu(self.bn7(self.conv7(concat2)))\n",
    "\n",
    "        # Concat out5 (resized) con out7\n",
    "        out5_resized = F.interpolate(out5, size=out7.shape[2:])\n",
    "        concat3 = torch.cat((out7, out5_resized), dim=1)  # [B, 224+160=384, 7, 7]\n",
    "        out8 = F.relu(self.bn8(self.conv8(concat3)))\n",
    "\n",
    "        x = self.global_pool(out8)  # [B, 256, 4, 4]\n",
    "        x = x.view(x.size(0), -1)   # Flatten → [B, 4096]\n",
    "\n",
    "        x = self.dropout(F.relu(self.fc1(x)))  # 4096 → 512\n",
    "        out = self.fc2(x)  # 512 → num_classes\n",
    "\n",
    "        return out\n"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Dueling"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T19:30:40.171543Z",
     "start_time": "2025-06-18T19:30:40.166830Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from stable_baselines3.dqn.policies import DQNPolicy\n",
    "class DuelingCnnExtractor(BaseFeaturesExtractor):\n",
    "    def __init__(self, observation_space, features_dim=512):\n",
    "        super().__init__(observation_space, features_dim)\n",
    "        n_input_channels = observation_space.shape[0]\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(n_input_channels, 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            n_flatten = self.cnn(torch.zeros(1, *observation_space.shape)).shape[1]\n",
    "\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(n_flatten, features_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, obs):\n",
    "        return self.linear(self.cnn(obs))\n",
    "\n",
    "class DuelingDQNPolicy(DQNPolicy):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(\n",
    "            *args,\n",
    "            features_extractor_class=DuelingCnnExtractor,\n",
    "            **kwargs\n",
    "        )\n",
    "        # Rebuild Q network with dueling architecture\n",
    "        features_dim = self.q_net.q_net[0].in_features\n",
    "        action_dim = self.action_space.n\n",
    "\n",
    "        self.q_net = nn.Sequential(\n",
    "            nn.Linear(features_dim, 512),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.q_net_adv = nn.Linear(512, action_dim)\n",
    "        self.q_net_val = nn.Linear(512, 1)\n",
    "\n",
    "    def forward(self, obs, deterministic=False):\n",
    "        features = self.extract_features(obs, features_extractor=DuelingCnnExtractor)\n",
    "        x = self.q_net(features)\n",
    "        adv = self.q_net_adv(x)\n",
    "        val = self.q_net_val(x)\n",
    "        q_values = val + adv - adv.mean(dim=1, keepdim=True)\n",
    "        return q_values"
   ],
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-21T03:23:36.779688Z",
     "start_time": "2025-06-21T03:23:36.769623Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class DeepMindCNN(BaseFeaturesExtractor):\n",
    "    \"\"\"\n",
    "    DeepMind-style CNN used in the original DQN paper (Mnih et al., 2015).\n",
    "    Input shape: (n_stack, 84, 84) → (4, 84, 84)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, observation_space, features_dim=512):\n",
    "        # features_dim is the output of the last linear layer (fc1)\n",
    "        super().__init__(observation_space, features_dim)\n",
    "\n",
    "        # Check input shape\n",
    "        n_input_channels = observation_space.shape[2]  # e.g., 4 stacked grayscale frames\n",
    "\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(n_input_channels, 32, kernel_size=8, stride=4),  # (32, 20, 20)\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),                 # (64, 9, 9)\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),                 # (64, 7, 7)\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            sample_input = torch.as_tensor(observation_space.sample()[None]).float()\n",
    "            sample_input = self._preprocess(sample_input)  # Preprocess the input\n",
    "            n_flatten = self.cnn(sample_input).shape[1]\n",
    "\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(n_flatten , n_flatten // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(n_flatten // 2, n_flatten // 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(n_flatten // 4, features_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self._features_dim = features_dim\n",
    "\n",
    "    def _preprocess(self, x):\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        return x\n",
    "\n",
    "    def forward(self, observations: torch.Tensor) -> torch.Tensor:\n",
    "        x = self._preprocess(observations)\n",
    "        x = self.cnn(x)\n",
    "        return self.linear(x)"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OB9-_5HPGb2b"
   },
   "source": [
    "2. Implementación de la solución DQN"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Nota**: Las primeras pruebas fueron sin usar el AtariWrapper, pero se preprocesaba internamente en las capas."
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-21T03:23:38.876399Z",
     "start_time": "2025-06-21T03:23:38.866337Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TQDMProgressCallback(BaseCallback):\n",
    "    def __init__(self, total_timesteps: int, verbose=0):\n",
    "        super().__init__(verbose)\n",
    "        self.total_timesteps = total_timesteps\n",
    "        self.progress_bar = None\n",
    "        self.last_timesteps = 0\n",
    "\n",
    "    def _on_training_start(self):\n",
    "        self.progress_bar = tqdm(total=self.total_timesteps, desc=\"Training Progress\", unit=\"step\")\n",
    "\n",
    "    def _on_step(self):\n",
    "        steps_since_last = self.num_timesteps - self.last_timesteps\n",
    "        self.progress_bar.update(steps_since_last)\n",
    "        self.last_timesteps += 1\n",
    "\n",
    "        # Optional: log latest reward if available\n",
    "        infos = self.locals.get(\"infos\", [])\n",
    "        if infos and isinstance(infos[0], dict) and \"episode\" in infos[0]:\n",
    "            self.progress_bar.set_postfix(reward=infos[0][\"episode\"][\"r\"])\n",
    "        return True  # Return True to continue training\n",
    "\n",
    "    def _on_training_end(self):\n",
    "        self.progress_bar.close()"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-21T03:23:40.631841Z",
     "start_time": "2025-06-21T03:23:40.621539Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "class MLflowCallback(BaseCallback):\n",
    "    def __init__(self, best_model_path, experiment_name=\"SB3_Experiment\", run_name=None, log_freq=1000, verbose=0):\n",
    "        super().__init__(verbose)\n",
    "        self.experiment_name = experiment_name\n",
    "        self.log_freq = log_freq\n",
    "        self.step_count = 0\n",
    "        self.best_mean_reward = -np.inf\n",
    "        self.best_model_path = best_model_path\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        self.step_count += 1\n",
    "        if self.step_count % self.log_freq == 0:\n",
    "            rewards = [ep_info['r'] for ep_info in self.model.ep_info_buffer] if self.model.ep_info_buffer else []\n",
    "            lengths = [ep_info['l'] for ep_info in self.model.ep_info_buffer] if self.model.ep_info_buffer else []\n",
    "\n",
    "            mean_reward = np.mean(rewards) if rewards else 0.0\n",
    "            max_reward = np.max(rewards) if rewards else 0.0\n",
    "            min_reward = np.min(rewards) if rewards else 0.0\n",
    "            mean_length = np.mean(lengths) if lengths else 0.0\n",
    "            std_reward = np.std(rewards) if rewards else 0.0\n",
    "\n",
    "            step = self.num_timesteps\n",
    "            mlflow.log_metric(\"timesteps\", step, step=step)\n",
    "            mlflow.log_metric(\"episode_reward_mean\", mean_reward, step=step)\n",
    "            mlflow.log_metric(\"episode_reward_max\", max_reward, step=step)\n",
    "            mlflow.log_metric(\"episode_reward_min\", min_reward, step=step)\n",
    "            mlflow.log_metric(\"episode_length_mean\", mean_length, step=step)\n",
    "            mlflow.log_metric(\"episode_reward_std\", std_reward, step=step)\n",
    "\n",
    "            if mean_reward > self.best_mean_reward:\n",
    "                self.best_mean_reward = mean_reward\n",
    "                # Save the best model\n",
    "                self.model.save(self.best_model_path)\n",
    "        return True\n",
    "\n",
    "    def _on_training_end(self):\n",
    "        # Optionally save the model as artifact\n",
    "        mlflow.log_param(\"num_episodes\", len(self.model.ep_info_buffer))\n",
    "        mlflow.end_run()"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-21T17:05:20.193682Z",
     "start_time": "2025-06-21T17:05:20.183492Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from collections import Counter\n",
    "\n",
    "class TestCallBack(BaseCallback):\n",
    "    def __init__(self, env, n_episodes=100, verbose=0, test_timesteps=10000):\n",
    "        super().__init__(verbose)\n",
    "        self.env = env\n",
    "        self.n_episodes = n_episodes\n",
    "        self.rewards = []\n",
    "        self.test_timesteps = test_timesteps\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.num_timesteps % self.test_timesteps == 0:  # Test every 1000 steps\n",
    "            action_counter = Counter()\n",
    "            for _ in range(self.n_episodes):\n",
    "                ep_reward = 0\n",
    "                obs = self.env.reset()\n",
    "                done = False\n",
    "                while not done:\n",
    "                    with torch.no_grad():\n",
    "                        action, _ = self.model.predict(obs)\n",
    "                    obs, reward, done, _ = self.env.step(action)\n",
    "                    action_scalar = int(action)\n",
    "                    action_counter[action_scalar] += 1\n",
    "                    ep_reward += reward\n",
    "                self.rewards.append(ep_reward)\n",
    "            mean_reward = np.mean(self.rewards)\n",
    "            std_reward = np.std(self.rewards)\n",
    "            mlflow.log_metric(\"test_reward\", mean_reward, step=self.num_timesteps)\n",
    "            mlflow.log_metric(\"test_reward_std\", std_reward, step=self.num_timesteps)\n",
    "            total_actions = sum(action_counter.values())\n",
    "            for action, count in action_counter.items():\n",
    "                mlflow.log_metric(f\"action_{action}_count\", count, step=self.num_timesteps)\n",
    "                mlflow.log_metric(f\"action_{action}_percentage\", count / total_actions, step=self.num_timesteps)\n",
    "        return True\n"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Resnet152"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T23:39:33.016477Z",
     "start_time": "2025-06-15T22:25:35.158596Z"
    }
   },
   "cell_type": "code",
   "source": [
    "policy_kwargs = dict(\n",
    "    features_extractor_class=ResNet152FeaturesExtractor,\n",
    "    features_extractor_kwargs=dict(features_dim=256),\n",
    ")\n",
    "total_timesteps = 100000\n",
    "progress_bar_callback = TQDMProgressCallback(total_timesteps=total_timesteps)\n",
    "ml_callback = MLflowCallback(\n",
    "    \"models/dqn_resnet152_weights.zip\",\n",
    "    experiment_name=\"DQN_SpaceInvaders\",\n",
    "    run_name=\"DQN_Run\",\n",
    "    log_freq=500\n",
    ")\n",
    "experiment_name = \"DQN_SpaceInvaders\"\n",
    "exist_experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "if not exist_experiment:\n",
    "    mlflow.create_experiment(experiment_name)\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "with mlflow.start_run(run_name=\"DQN_Run_ResNet152_finetuned\"):\n",
    "    mlflow.log_param(\"env_name\", env_name)\n",
    "    mlflow.log_param(\"total_timesteps\", total_timesteps)\n",
    "    mlflow.log_param(\"learning_rate\", 1e-4)\n",
    "    mlflow.log_param(\"buffer_size\", 100000)\n",
    "\n",
    "    model = DQN(\"CnnPolicy\", env, verbose=1, learning_rate=1e-4, buffer_size=100000, policy_kwargs=policy_kwargs)\n",
    "    t_model = model.learn(total_timesteps=total_timesteps, callback=[progress_bar_callback, ml_callback, TestCallBack(env)])\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Training Progress:   0%|          | 0/100000 [00:00<?, ?step/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c499d7db03bd43a0af113b4682d09769"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 762      |\n",
      "|    ep_rew_mean      | 10       |\n",
      "|    exploration_rate | 0.711    |\n",
      "| time/               |          |\n",
      "|    episodes         | 4        |\n",
      "|    fps              | 22       |\n",
      "|    time_elapsed     | 136      |\n",
      "|    total_timesteps  | 3046     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0128   |\n",
      "|    n_updates        | 736      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 907      |\n",
      "|    ep_rew_mean      | 13.5     |\n",
      "|    exploration_rate | 0.311    |\n",
      "| time/               |          |\n",
      "|    episodes         | 8        |\n",
      "|    fps              | 28       |\n",
      "|    time_elapsed     | 253      |\n",
      "|    total_timesteps  | 7254     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.009    |\n",
      "|    n_updates        | 1788     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 832      |\n",
      "|    ep_rew_mean      | 11.9     |\n",
      "|    exploration_rate | 0.0518   |\n",
      "| time/               |          |\n",
      "|    episodes         | 12       |\n",
      "|    fps              | 23       |\n",
      "|    time_elapsed     | 422      |\n",
      "|    total_timesteps  | 9981     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000445 |\n",
      "|    n_updates        | 2470     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 781      |\n",
      "|    ep_rew_mean      | 11.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 16       |\n",
      "|    fps              | 20       |\n",
      "|    time_elapsed     | 615      |\n",
      "|    total_timesteps  | 12501    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00241  |\n",
      "|    n_updates        | 3100     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 775      |\n",
      "|    ep_rew_mean      | 11.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 20       |\n",
      "|    fps              | 21       |\n",
      "|    time_elapsed     | 717      |\n",
      "|    total_timesteps  | 15509    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0135   |\n",
      "|    n_updates        | 3852     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 775      |\n",
      "|    ep_rew_mean      | 11.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 24       |\n",
      "|    fps              | 22       |\n",
      "|    time_elapsed     | 822      |\n",
      "|    total_timesteps  | 18597    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00135  |\n",
      "|    n_updates        | 4624     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 773      |\n",
      "|    ep_rew_mean      | 11.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 28       |\n",
      "|    fps              | 21       |\n",
      "|    time_elapsed     | 1015     |\n",
      "|    total_timesteps  | 21651    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0119   |\n",
      "|    n_updates        | 5387     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 768      |\n",
      "|    ep_rew_mean      | 10.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 32       |\n",
      "|    fps              | 22       |\n",
      "|    time_elapsed     | 1115     |\n",
      "|    total_timesteps  | 24581    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00243  |\n",
      "|    n_updates        | 6120     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 757      |\n",
      "|    ep_rew_mean      | 10.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 36       |\n",
      "|    fps              | 22       |\n",
      "|    time_elapsed     | 1205     |\n",
      "|    total_timesteps  | 27235    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0414   |\n",
      "|    n_updates        | 6783     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 744      |\n",
      "|    ep_rew_mean      | 10.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 40       |\n",
      "|    fps              | 23       |\n",
      "|    time_elapsed     | 1291     |\n",
      "|    total_timesteps  | 29759    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00338  |\n",
      "|    n_updates        | 7414     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 728      |\n",
      "|    ep_rew_mean      | 10.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 44       |\n",
      "|    fps              | 21       |\n",
      "|    time_elapsed     | 1457     |\n",
      "|    total_timesteps  | 32040    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0184   |\n",
      "|    n_updates        | 7984     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 732      |\n",
      "|    ep_rew_mean      | 10.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 48       |\n",
      "|    fps              | 22       |\n",
      "|    time_elapsed     | 1561     |\n",
      "|    total_timesteps  | 35115    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00615  |\n",
      "|    n_updates        | 8753     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 744      |\n",
      "|    ep_rew_mean      | 10.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 52       |\n",
      "|    fps              | 23       |\n",
      "|    time_elapsed     | 1681     |\n",
      "|    total_timesteps  | 38668    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00368  |\n",
      "|    n_updates        | 9641     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 733      |\n",
      "|    ep_rew_mean      | 10.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 56       |\n",
      "|    fps              | 21       |\n",
      "|    time_elapsed     | 1876     |\n",
      "|    total_timesteps  | 41059    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00603  |\n",
      "|    n_updates        | 10239    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 733      |\n",
      "|    ep_rew_mean      | 10.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 60       |\n",
      "|    fps              | 22       |\n",
      "|    time_elapsed     | 1975     |\n",
      "|    total_timesteps  | 43998    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.003    |\n",
      "|    n_updates        | 10974    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 737      |\n",
      "|    ep_rew_mean      | 10.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 64       |\n",
      "|    fps              | 22       |\n",
      "|    time_elapsed     | 2082     |\n",
      "|    total_timesteps  | 47139    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0339   |\n",
      "|    n_updates        | 11759    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 735      |\n",
      "|    ep_rew_mean      | 10.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 68       |\n",
      "|    fps              | 21       |\n",
      "|    time_elapsed     | 2280     |\n",
      "|    total_timesteps  | 50001    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00315  |\n",
      "|    n_updates        | 12475    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 743      |\n",
      "|    ep_rew_mean      | 10.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 72       |\n",
      "|    fps              | 22       |\n",
      "|    time_elapsed     | 2399     |\n",
      "|    total_timesteps  | 53529    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0165   |\n",
      "|    n_updates        | 13357    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 751      |\n",
      "|    ep_rew_mean      | 11.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 76       |\n",
      "|    fps              | 22       |\n",
      "|    time_elapsed     | 2520     |\n",
      "|    total_timesteps  | 57102    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00212  |\n",
      "|    n_updates        | 14250    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 759      |\n",
      "|    ep_rew_mean      | 11.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 80       |\n",
      "|    fps              | 22       |\n",
      "|    time_elapsed     | 2743     |\n",
      "|    total_timesteps  | 60690    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0231   |\n",
      "|    n_updates        | 15147    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 760      |\n",
      "|    ep_rew_mean      | 11.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 84       |\n",
      "|    fps              | 22       |\n",
      "|    time_elapsed     | 2849     |\n",
      "|    total_timesteps  | 63811    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.022    |\n",
      "|    n_updates        | 15927    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 751      |\n",
      "|    ep_rew_mean      | 11.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 88       |\n",
      "|    fps              | 22       |\n",
      "|    time_elapsed     | 2927     |\n",
      "|    total_timesteps  | 66121    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00219  |\n",
      "|    n_updates        | 16505    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 758      |\n",
      "|    ep_rew_mean      | 11.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 92       |\n",
      "|    fps              | 22       |\n",
      "|    time_elapsed     | 3048     |\n",
      "|    total_timesteps  | 69719    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0037   |\n",
      "|    n_updates        | 17404    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 746      |\n",
      "|    ep_rew_mean      | 11.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 96       |\n",
      "|    fps              | 22       |\n",
      "|    time_elapsed     | 3184     |\n",
      "|    total_timesteps  | 71623    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00906  |\n",
      "|    n_updates        | 17880    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 750      |\n",
      "|    ep_rew_mean      | 11.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 100      |\n",
      "|    fps              | 22       |\n",
      "|    time_elapsed     | 3299     |\n",
      "|    total_timesteps  | 75041    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00518  |\n",
      "|    n_updates        | 18735    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 742      |\n",
      "|    ep_rew_mean      | 11.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 104      |\n",
      "|    fps              | 22       |\n",
      "|    time_elapsed     | 3375     |\n",
      "|    total_timesteps  | 77292    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00344  |\n",
      "|    n_updates        | 19297    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 727      |\n",
      "|    ep_rew_mean      | 11.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 108      |\n",
      "|    fps              | 22       |\n",
      "|    time_elapsed     | 3571     |\n",
      "|    total_timesteps  | 80001    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00534  |\n",
      "|    n_updates        | 19975    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 728      |\n",
      "|    ep_rew_mean      | 11.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 112      |\n",
      "|    fps              | 22       |\n",
      "|    time_elapsed     | 3663     |\n",
      "|    total_timesteps  | 82743    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0186   |\n",
      "|    n_updates        | 20660    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 731      |\n",
      "|    ep_rew_mean      | 11.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 116      |\n",
      "|    fps              | 22       |\n",
      "|    time_elapsed     | 3762     |\n",
      "|    total_timesteps  | 85646    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0114   |\n",
      "|    n_updates        | 21386    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 730      |\n",
      "|    ep_rew_mean      | 11.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 120      |\n",
      "|    fps              | 22       |\n",
      "|    time_elapsed     | 3859     |\n",
      "|    total_timesteps  | 88523    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.024    |\n",
      "|    n_updates        | 22105    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 718      |\n",
      "|    ep_rew_mean      | 11.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 124      |\n",
      "|    fps              | 22       |\n",
      "|    time_elapsed     | 4011     |\n",
      "|    total_timesteps  | 90372    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00466  |\n",
      "|    n_updates        | 22567    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 712      |\n",
      "|    ep_rew_mean      | 11.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 128      |\n",
      "|    fps              | 22       |\n",
      "|    time_elapsed     | 4097     |\n",
      "|    total_timesteps  | 92885    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00812  |\n",
      "|    n_updates        | 23196    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 712      |\n",
      "|    ep_rew_mean      | 11.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 132      |\n",
      "|    fps              | 22       |\n",
      "|    time_elapsed     | 4198     |\n",
      "|    total_timesteps  | 95801    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00455  |\n",
      "|    n_updates        | 23925    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 712      |\n",
      "|    ep_rew_mean      | 11.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 136      |\n",
      "|    fps              | 22       |\n",
      "|    time_elapsed     | 4287     |\n",
      "|    total_timesteps  | 98436    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0063   |\n",
      "|    n_updates        | 24583    |\n",
      "----------------------------------\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### MobileNetV2"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "foSlxWH1Gb2b"
   },
   "source": [
    "policy_kwargs = dict(\n",
    "    features_extractor_class=MobileNetFeatureExtractor,\n",
    "    features_extractor_kwargs=dict(features_dim=256),\n",
    ")\n",
    "total_timesteps = 2_000_000\n",
    "progress_bar_callback = TQDMProgressCallback(total_timesteps=total_timesteps)\n",
    "ml_callback = MLflowCallback(\n",
    "    \"models/dqn_mobilenet_v2_weights_new_data.zip\",\n",
    "    experiment_name=\"DQN_SpaceInvaders\",\n",
    "    run_name=\"DQN_Run\",\n",
    "    log_freq=10_000\n",
    ")\n",
    "experiment_name = \"DQN_SpaceInvaders\"\n",
    "exist_experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "if not exist_experiment:\n",
    "    mlflow.create_experiment(experiment_name)\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "with mlflow.start_run(run_name=\"DQN_Run_MobileNetv2_finetuned_new_data\"):\n",
    "    mlflow.log_param(\"env_name\", env_name)\n",
    "    mlflow.log_param(\"total_timesteps\", total_timesteps)\n",
    "    mlflow.log_param(\"learning_rate\", 1e-4)\n",
    "    mlflow.log_param(\"buffer_size\", 500_000)\n",
    "\n",
    "    model = DQN(\"CnnPolicy\", env, verbose=0, learning_rate=1e-4, buffer_size=500_000, policy_kwargs=policy_kwargs)\n",
    "    t_model = model.learn(total_timesteps=total_timesteps, callback=[progress_bar_callback, ml_callback, TestCallBack(env, test_timesteps=100_000)])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### ViT B-16"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Cargar pesos preentrenados"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "total_timesteps = 100000\n",
    "progress_bar_callback = TQDMProgressCallback(total_timesteps=total_timesteps)\n",
    "ml_callback = MLflowCallback(\n",
    "    best_model_path=\"models/dqn_vit_b_16_weights.zip\",\n",
    "    experiment_name=\"DQN_SpaceInvaders\",\n",
    "    run_name=\"DQN_Run\",\n",
    "    log_freq=500\n",
    ")\n",
    "experiment_name = \"DQN_SpaceInvaders\"\n",
    "exist_experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "if not exist_experiment:\n",
    "    mlflow.create_experiment(experiment_name)\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "with mlflow.start_run(run_name=\"DQN_Run_Vit_b_16_finetuned\"):\n",
    "    mlflow.log_param(\"env_name\", env_name)\n",
    "    mlflow.log_param(\"total_timesteps\", total_timesteps)\n",
    "    mlflow.log_param(\"learning_rate\", 1e-4)\n",
    "    mlflow.log_param(\"buffer_size\", 100000)\n",
    "    model = DQN.load(\"models/dqn_vit_b_16_weights.zip\", env=env)\n",
    "    t_model = model.learn(total_timesteps=total_timesteps, callback=[progress_bar_callback, ml_callback])\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "policy_kwargs = dict(\n",
    "    features_extractor_class=VitB16FeaturesExtractor,\n",
    "    features_extractor_kwargs=dict(features_dim=256),\n",
    ")\n",
    "total_timesteps = 100000\n",
    "progress_bar_callback = TQDMProgressCallback(total_timesteps=total_timesteps)\n",
    "ml_callback = MLflowCallback(\n",
    "    best_model_path=\"models/dqn_vit_b_16_weights_upscaled.zip\",\n",
    "    experiment_name=\"DQN_SpaceInvaders\",\n",
    "    run_name=\"DQN_Run\",\n",
    "    log_freq=500\n",
    ")\n",
    "experiment_name = \"DQN_SpaceInvaders\"\n",
    "exist_experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "if not exist_experiment:\n",
    "    mlflow.create_experiment(experiment_name)\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "with mlflow.start_run(run_name=\"DQN_Run_Vit_b_16_gray_upscaled\"):\n",
    "    mlflow.log_param(\"env_name\", env_name)\n",
    "    mlflow.log_param(\"total_timesteps\", total_timesteps)\n",
    "    mlflow.log_param(\"learning_rate\", 1e-4)\n",
    "    mlflow.log_param(\"buffer_size\", 100000)\n",
    "\n",
    "    model = DQN(\"CnnPolicy\", env, verbose=0, learning_rate=1e-4, buffer_size=100000, policy_kwargs=policy_kwargs)\n",
    "    t_model = model.learn(total_timesteps=total_timesteps, callback=[progress_bar_callback, ml_callback, TestCallBack(env)])\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### CNNConnectedDeep"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Por el momento es el mejor modelo por su entrenamiento y resultados iniciales, además de velocidad."
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T04:11:41.945058Z",
     "start_time": "2025-06-18T03:57:27.142497Z"
    }
   },
   "cell_type": "code",
   "source": [
    "policy_kwargs = dict(\n",
    "    features_extractor_class=CNNConnectedDeep,\n",
    "    features_extractor_kwargs=dict(features_dim=256)\n",
    ")\n",
    "total_timesteps = 100000\n",
    "progress_bar_callback = TQDMProgressCallback(total_timesteps=total_timesteps)\n",
    "ml_callback = MLflowCallback(\n",
    "    best_model_path=\"models/dqn_cnn_connected_deep_weights.zip\",\n",
    "    experiment_name=\"DQN_SpaceInvaders\",\n",
    "    run_name=\"DQN_Run\",\n",
    "    log_freq=1000\n",
    ")\n",
    "experiment_name = \"DQN_SpaceInvaders\"\n",
    "exist_experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "if not exist_experiment:\n",
    "    mlflow.create_experiment(experiment_name)\n",
    "mlflow.set_experiment(experiment_name)\n",
    "with mlflow.start_run(run_name=\"DQN_Run_CNNConnectedDeep\"):\n",
    "    mlflow.log_param(\"env_name\", env_name)\n",
    "    mlflow.log_param(\"total_timesteps\", total_timesteps)\n",
    "    mlflow.log_param(\"learning_rate\", 1e-4)\n",
    "    mlflow.log_param(\"buffer_size\", 100000)\n",
    "\n",
    "    model = DQN(\"CnnPolicy\", env, verbose=0, learning_rate=1e-4, buffer_size=100000, policy_kwargs=policy_kwargs)\n",
    "    t_model = model.learn(total_timesteps=total_timesteps, callback=[progress_bar_callback, ml_callback, TestCallBack(env, test_timesteps=5000)])"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Training Progress:   0%|          | 0/100000 [00:00<?, ?step/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f7334addc02f48da9ef7ff04c6041fe0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3. Fine tuning de los modelos"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### CCNConnectedDeep\n",
    "con nuevas dimensiones de 84,84"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-06-18T04:47:46.533395Z"
    }
   },
   "cell_type": "code",
   "source": [
    "policy_kwargs = dict(\n",
    "    features_extractor_class=CNNConnectedDeep,\n",
    "    features_extractor_kwargs=dict(features_dim=256)\n",
    ")\n",
    "total_timesteps = 5_000_000\n",
    "progress_bar_callback = TQDMProgressCallback(total_timesteps=total_timesteps)\n",
    "ml_callback = MLflowCallback(\n",
    "    best_model_path=\"models/dqn_cnn_connected_deep_weights_finetuning.zip\",\n",
    "    experiment_name=\"DQN_SpaceInvaders\",\n",
    "    run_name=\"DQN_Run\",\n",
    "    log_freq=25000\n",
    ")\n",
    "\n",
    "experiment_name = \"DQN_SpaceInvaders\"\n",
    "exist_experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "if not exist_experiment:\n",
    "    mlflow.create_experiment(experiment_name)\n",
    "mlflow.set_experiment(experiment_name)\n",
    "with mlflow.start_run(run_name=\"DQN_Run_CNNConnectedDeep_finetuned\"):\n",
    "    mlflow.log_param(\"env_name\", env_name)\n",
    "    mlflow.log_param(\"total_timesteps\", total_timesteps)\n",
    "    mlflow.log_param(\"learning_rate\", 1e-3)\n",
    "    mlflow.log_param(\"buffer_size\", 100_000)\n",
    "    mlflow.log_param(\"exploration_fraction\", 0.1)\n",
    "    mlflow.log_param(\"exploration_final_eps\", 0.1)\n",
    "    mlflow.log_param(\"exploration_initial_eps\", 1.0)\n",
    "    mlflow.log_param(\"batch_size\", 32)\n",
    "    mlflow.log_param(\"gamma\", 0.99)\n",
    "    mlflow.log_param(\"learning_starts\", 10000)\n",
    "    mlflow.log_param(\"target_update_interval\", 20000)\n",
    "\n",
    "    model = DQN(\"CnnPolicy\", env, verbose=0,\n",
    "                learning_rate=1e-4, buffer_size=300_000,\n",
    "                policy_kwargs=policy_kwargs,\n",
    "                exploration_fraction=0.3, exploration_initial_eps=1.0,\n",
    "                exploration_final_eps=0.01, target_update_interval=20000,\n",
    "                batch_size=32, learning_starts=100_000, gamma=0.99,seed=23)\n",
    "    t_model = model.learn(total_timesteps=total_timesteps, callback=[progress_bar_callback, ml_callback, TestCallBack(env, test_timesteps=50_000)])"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Training Progress:   0%|          | 0/5000000 [00:00<?, ?step/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "33722a18bb144c0f82c44b1ceac2cbb8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### DeppMind\n",
    "\n",
    "- Ya viene por efecto en stable-Baselines3"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "policy_kwargs = dict(\n",
    "    net_arch = [256, 256],\n",
    ")\n",
    "total_timesteps = 100000\n",
    "progress_bar_callback = TQDMProgressCallback(total_timesteps=total_timesteps)\n",
    "ml_callback = MLflowCallback(\n",
    "    best_model_path=\"models/deep_mind_data.zip\",\n",
    "    experiment_name=\"DQN_SpaceInvaders\",\n",
    "    run_name=\"DQN_Run\",\n",
    "    log_freq=1000\n",
    ")\n",
    "experiment_name = \"DQN_SpaceInvaders\"\n",
    "exist_experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "if not exist_experiment:\n",
    "    mlflow.create_experiment(experiment_name)\n",
    "mlflow.set_experiment(experiment_name)\n",
    "with mlflow.start_run(run_name=\"deep_mind\"):\n",
    "    mlflow.log_param(\"env_name\", env_name)\n",
    "    mlflow.log_param(\"total_timesteps\", total_timesteps)\n",
    "    mlflow.log_param(\"learning_rate\", 1e-4)\n",
    "    mlflow.log_param(\"buffer_size\", 100000)\n",
    "\n",
    "    model = DQN(\"CnnPolicy\", env, verbose=0, learning_rate=1e-4, buffer_size=100000, policy_kwargs=policy_kwargs)\n",
    "    t_model = model.learn(total_timesteps=total_timesteps, callback=[progress_bar_callback, ml_callback, TestCallBack(env, test_timesteps=5000)])\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T19:11:13.245353Z",
     "start_time": "2025-06-18T14:11:08.771654Z"
    }
   },
   "cell_type": "code",
   "source": [
    "old_model = DQN.load(\"models/deep_mind_data.zip\", env=env)\n",
    "\n",
    "policy_kwargs = dict(\n",
    "    net_arch = [256, 256],\n",
    ")\n",
    "\n",
    "total_timesteps = 5_000_000\n",
    "progress_bar_callback = TQDMProgressCallback(total_timesteps=total_timesteps)\n",
    "ml_callback = MLflowCallback(\n",
    "    best_model_path=\"models/deep_mind_finetuning.zip\",\n",
    "    experiment_name=\"DQN_SpaceInvaders\",\n",
    "    run_name=\"DQN_Run\",\n",
    "    log_freq=25000\n",
    ")\n",
    "\n",
    "experiment_name = \"DQN_SpaceInvaders\"\n",
    "exist_experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "if not exist_experiment:\n",
    "    mlflow.create_experiment(experiment_name)\n",
    "mlflow.set_experiment(experiment_name)\n",
    "with mlflow.start_run(run_name=\"DQN_Run_deep_mind_finetuned\"):\n",
    "    mlflow.log_param(\"env_name\", env_name)\n",
    "    mlflow.log_param(\"total_timesteps\", total_timesteps)\n",
    "    mlflow.log_param(\"learning_rate\", 1e-3)\n",
    "    mlflow.log_param(\"buffer_size\", 100_000)\n",
    "    mlflow.log_param(\"exploration_fraction\", 0.2)\n",
    "    mlflow.log_param(\"exploration_final_eps\", 0.01)\n",
    "    mlflow.log_param(\"exploration_initial_eps\", 1.0)\n",
    "    mlflow.log_param(\"batch_size\", 32)\n",
    "    mlflow.log_param(\"gamma\", 0.99)\n",
    "    mlflow.log_param(\"learning_starts\", 10000)\n",
    "    mlflow.log_param(\"target_update_interval\", 20000)\n",
    "\n",
    "    model = DQN(\"CnnPolicy\", env, verbose=0,\n",
    "                policy_kwargs=policy_kwargs,\n",
    "                learning_rate=1e-4, buffer_size=300_000,\n",
    "                exploration_fraction=0.2, exploration_initial_eps=1.0,\n",
    "                exploration_final_eps=0.01, target_update_interval=1000,\n",
    "                batch_size=32, learning_starts=100_000, gamma=0.99,seed=23)\n",
    "    model.policy.load_state_dict(old_model.policy.state_dict())\n",
    "\n",
    "    t_model = model.learn(total_timesteps=total_timesteps, callback=[progress_bar_callback, ml_callback, TestCallBack(env, test_timesteps=50_000)])\n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Training Progress:   0%|          | 0/5000000 [00:00<?, ?step/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "51c880ccae6a4a6795b111c9c7aa0602"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Tuning con dueling"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T04:53:50.284674Z",
     "start_time": "2025-06-18T21:12:49.550742Z"
    }
   },
   "cell_type": "code",
   "source": [
    "policy_kwargs = dict(\n",
    "    net_arch = [256, 256],\n",
    ")\n",
    "total_timesteps = 5_000_000\n",
    "progress_bar_callback = TQDMProgressCallback(total_timesteps=total_timesteps)\n",
    "ml_callback = MLflowCallback(\n",
    "    best_model_path=\"models/duel.zip\",\n",
    "    experiment_name=\"DQN_SpaceInvaders\",\n",
    "    run_name=\"DQN_Run\",\n",
    "    log_freq=25_000\n",
    ")\n",
    "\n",
    "experiment_name = \"DQN_SpaceInvaders\"\n",
    "exist_experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "if not exist_experiment:\n",
    "    mlflow.create_experiment(experiment_name)\n",
    "mlflow.set_experiment(experiment_name)\n",
    "with mlflow.start_run(run_name=\"duel\"):\n",
    "    mlflow.log_param(\"env_name\", env_name)\n",
    "    mlflow.log_param(\"total_timesteps\", total_timesteps)\n",
    "    mlflow.log_param(\"learning_rate\", 1e-3)\n",
    "    model = QRDQN(\"CnnPolicy\", env, verbose=0, learning_rate=1e-3, buffer_size=1_000_000, policy_kwargs=policy_kwargs, seed=23)\n",
    "\n",
    "    t_model = model.learn(total_timesteps=total_timesteps, callback=[progress_bar_callback, ml_callback, TestCallBack(env, test_timesteps=50_000)])\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\migue\\anaconda3\\envs\\pytorch\\lib\\site-packages\\stable_baselines3\\common\\buffers.py:242: UserWarning: This system does not have apparently enough memory to store the complete replay buffer 14.13GB > 12.19GB\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Training Progress:   0%|          | 0/5000000 [00:00<?, ?step/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ad770a63b7f54b67bedd6f2415bf2986"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\migue\\anaconda3\\envs\\pytorch\\lib\\site-packages\\stable_baselines3\\common\\save_util.py:284: UserWarning: Path 'models' does not exist. Will create it.\n",
      "  warnings.warn(f\"Path '{path.parent}' does not exist. Will create it.\")\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-06-19T05:13:34.481291Z"
    }
   },
   "cell_type": "code",
   "source": [
    "total_timesteps = 20_000_000\n",
    "progress_bar_callback = TQDMProgressCallback(total_timesteps=total_timesteps)\n",
    "ml_callback = MLflowCallback(\n",
    "    best_model_path=\"models/duel_episodes.zip\",\n",
    "    experiment_name=\"DQN_SpaceInvaders\",\n",
    "    run_name=\"DQN_Run\",\n",
    "    log_freq=100_000\n",
    ")\n",
    "\n",
    "experiment_name = \"DQN_SpaceInvaders\"\n",
    "exist_experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "if not exist_experiment:\n",
    "    mlflow.create_experiment(experiment_name)\n",
    "mlflow.set_experiment(experiment_name)\n",
    "with mlflow.start_run(run_name=\"duel_more_episodes\"):\n",
    "    mlflow.log_param(\"env_name\", env_name)\n",
    "    mlflow.log_param(\"total_timesteps\", total_timesteps)\n",
    "    mlflow.log_param(\"learning_rate\", 25e-5)\n",
    "    mlflow.log_param(\"gamma\", 0.95)\n",
    "    mlflow.log_param(\"buffer_size\", 1_000_000)\n",
    "    mlflow.log_param(\"exploration_fraction\", 0.005)\n",
    "    mlflow.log_param(\"file_name\", \"models/duel_episodes.zip\")\n",
    "\n",
    "    model = QRDQN.load(\"models/duel.zip\", env=env)\n",
    "    model.learning_rate = 25e-5\n",
    "    model.gamma = 0.95\n",
    "    model.learning_starts = 100_000\n",
    "    t_model = model.learn(total_timesteps=total_timesteps, callback=[progress_bar_callback, ml_callback, TestCallBack(env, test_timesteps=200_000)])"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Training Progress:   0%|          | 0/20000000 [00:00<?, ?step/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "40392c870bf9467394906f5edeaa8e9d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-20T05:19:59.629822Z",
     "start_time": "2025-06-19T13:03:50.939762Z"
    }
   },
   "cell_type": "code",
   "source": [
    "total_timesteps = 20_000_000\n",
    "progress_bar_callback = TQDMProgressCallback(total_timesteps=total_timesteps)\n",
    "ml_callback = MLflowCallback(\n",
    "    best_model_path=\"models/duel_episodes_v2.zip\",\n",
    "    experiment_name=\"DQN_SpaceInvaders\",\n",
    "    run_name=\"DQN_Run\",\n",
    "    log_freq=100_000\n",
    ")\n",
    "\n",
    "experiment_name = \"DQN_SpaceInvaders\"\n",
    "exist_experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "if not exist_experiment:\n",
    "    mlflow.create_experiment(experiment_name)\n",
    "mlflow.set_experiment(experiment_name)\n",
    "with mlflow.start_run(run_name=\"duel_more_episodes\"):\n",
    "    mlflow.log_param(\"env_name\", env_name)\n",
    "    mlflow.log_param(\"total_timesteps\", total_timesteps)\n",
    "    mlflow.log_param(\"learning_rate\", 25e-5)\n",
    "    mlflow.log_param(\"gamma\", 0.95)\n",
    "    mlflow.log_param(\"buffer_size\", 1_000_000)\n",
    "    mlflow.log_param(\"exploration_fraction\", 0.005)\n",
    "    mlflow.log_param(\"file_name\", \"models/duel_episodes.zip\")\n",
    "\n",
    "    model = QRDQN.load(\"models/duel_episodes.zip\", env=env)\n",
    "    model.learning_rate = 25e-5\n",
    "    model.gamma = 0.95\n",
    "    model.learning_starts = 100_000\n",
    "    t_model = model.learn(total_timesteps=total_timesteps,\n",
    "                          callback=[progress_bar_callback, ml_callback, TestCallBack(env, test_timesteps=200_000)])\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\migue\\anaconda3\\envs\\pytorch\\lib\\site-packages\\stable_baselines3\\common\\buffers.py:242: UserWarning: This system does not have apparently enough memory to store the complete replay buffer 14.13GB > 11.42GB\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Training Progress:   0%|          | 0/20000000 [00:00<?, ?step/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bdb240f367434b4da51632c1edba91d4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[11], line 28\u001B[0m\n\u001B[0;32m     26\u001B[0m model\u001B[38;5;241m.\u001B[39mgamma \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.95\u001B[39m\n\u001B[0;32m     27\u001B[0m model\u001B[38;5;241m.\u001B[39mlearning_starts \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m100_000\u001B[39m\n\u001B[1;32m---> 28\u001B[0m t_model \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlearn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtotal_timesteps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtotal_timesteps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     29\u001B[0m \u001B[43m                      \u001B[49m\u001B[43mcallback\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[43mprogress_bar_callback\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mml_callback\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mTestCallBack\u001B[49m\u001B[43m(\u001B[49m\u001B[43menv\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_timesteps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m200_000\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\sb3_contrib\\qrdqn\\qrdqn.py:278\u001B[0m, in \u001B[0;36mQRDQN.learn\u001B[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001B[0m\n\u001B[0;32m    269\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mlearn\u001B[39m(\n\u001B[0;32m    270\u001B[0m     \u001B[38;5;28mself\u001B[39m: SelfQRDQN,\n\u001B[0;32m    271\u001B[0m     total_timesteps: \u001B[38;5;28mint\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    276\u001B[0m     progress_bar: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m    277\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m SelfQRDQN:\n\u001B[1;32m--> 278\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlearn\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    279\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtotal_timesteps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtotal_timesteps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    280\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcallback\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcallback\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    281\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlog_interval\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlog_interval\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    282\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtb_log_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtb_log_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    283\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreset_num_timesteps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreset_num_timesteps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    284\u001B[0m \u001B[43m        \u001B[49m\u001B[43mprogress_bar\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mprogress_bar\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    285\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\stable_baselines3\\common\\off_policy_algorithm.py:347\u001B[0m, in \u001B[0;36mOffPolicyAlgorithm.learn\u001B[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001B[0m\n\u001B[0;32m    345\u001B[0m         \u001B[38;5;66;03m# Special case when the user passes `gradient_steps=0`\u001B[39;00m\n\u001B[0;32m    346\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m gradient_steps \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m--> 347\u001B[0m             \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient_steps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgradient_steps\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    349\u001B[0m callback\u001B[38;5;241m.\u001B[39mon_training_end()\n\u001B[0;32m    351\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\sb3_contrib\\qrdqn\\qrdqn.py:228\u001B[0m, in \u001B[0;36mQRDQN.train\u001B[1;34m(self, gradient_steps, batch_size)\u001B[0m\n\u001B[0;32m    226\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmax_grad_norm \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    227\u001B[0m         th\u001B[38;5;241m.\u001B[39mnn\u001B[38;5;241m.\u001B[39mutils\u001B[38;5;241m.\u001B[39mclip_grad_norm_(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpolicy\u001B[38;5;241m.\u001B[39mparameters(), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmax_grad_norm)\n\u001B[1;32m--> 228\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpolicy\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptimizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    230\u001B[0m \u001B[38;5;66;03m# Increase update counter\u001B[39;00m\n\u001B[0;32m    231\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_n_updates \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m gradient_steps\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\optim\\optimizer.py:485\u001B[0m, in \u001B[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    480\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    481\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[0;32m    482\u001B[0m                 \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresult\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    483\u001B[0m             )\n\u001B[1;32m--> 485\u001B[0m out \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    486\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_optimizer_step_code()\n\u001B[0;32m    488\u001B[0m \u001B[38;5;66;03m# call optimizer step post hooks\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\optim\\optimizer.py:79\u001B[0m, in \u001B[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m     77\u001B[0m     torch\u001B[38;5;241m.\u001B[39mset_grad_enabled(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdefaults[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdifferentiable\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[0;32m     78\u001B[0m     torch\u001B[38;5;241m.\u001B[39m_dynamo\u001B[38;5;241m.\u001B[39mgraph_break()\n\u001B[1;32m---> 79\u001B[0m     ret \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m     80\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m     81\u001B[0m     torch\u001B[38;5;241m.\u001B[39m_dynamo\u001B[38;5;241m.\u001B[39mgraph_break()\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\optim\\adam.py:246\u001B[0m, in \u001B[0;36mAdam.step\u001B[1;34m(self, closure)\u001B[0m\n\u001B[0;32m    234\u001B[0m     beta1, beta2 \u001B[38;5;241m=\u001B[39m group[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbetas\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m    236\u001B[0m     has_complex \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_init_group(\n\u001B[0;32m    237\u001B[0m         group,\n\u001B[0;32m    238\u001B[0m         params_with_grad,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    243\u001B[0m         state_steps,\n\u001B[0;32m    244\u001B[0m     )\n\u001B[1;32m--> 246\u001B[0m     \u001B[43madam\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    247\u001B[0m \u001B[43m        \u001B[49m\u001B[43mparams_with_grad\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    248\u001B[0m \u001B[43m        \u001B[49m\u001B[43mgrads\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    249\u001B[0m \u001B[43m        \u001B[49m\u001B[43mexp_avgs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    250\u001B[0m \u001B[43m        \u001B[49m\u001B[43mexp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    251\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmax_exp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    252\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstate_steps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    253\u001B[0m \u001B[43m        \u001B[49m\u001B[43mamsgrad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mamsgrad\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    254\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhas_complex\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhas_complex\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    255\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbeta1\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta1\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    256\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbeta2\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta2\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    257\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mlr\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    258\u001B[0m \u001B[43m        \u001B[49m\u001B[43mweight_decay\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mweight_decay\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    259\u001B[0m \u001B[43m        \u001B[49m\u001B[43meps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43meps\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    260\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmaximize\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmaximize\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    261\u001B[0m \u001B[43m        \u001B[49m\u001B[43mforeach\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mforeach\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    262\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcapturable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcapturable\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    263\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdifferentiable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mdifferentiable\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    264\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfused\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mfused\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    265\u001B[0m \u001B[43m        \u001B[49m\u001B[43mgrad_scale\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mgetattr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mgrad_scale\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    266\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfound_inf\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mgetattr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mfound_inf\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    267\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdecoupled_weight_decay\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mdecoupled_weight_decay\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    268\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    270\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m loss\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\optim\\optimizer.py:147\u001B[0m, in \u001B[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    145\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m disabled_func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    146\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 147\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\optim\\adam.py:933\u001B[0m, in \u001B[0;36madam\u001B[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, decoupled_weight_decay, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001B[0m\n\u001B[0;32m    930\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    931\u001B[0m     func \u001B[38;5;241m=\u001B[39m _single_tensor_adam\n\u001B[1;32m--> 933\u001B[0m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    934\u001B[0m \u001B[43m    \u001B[49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    935\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrads\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    936\u001B[0m \u001B[43m    \u001B[49m\u001B[43mexp_avgs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    937\u001B[0m \u001B[43m    \u001B[49m\u001B[43mexp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    938\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmax_exp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    939\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstate_steps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    940\u001B[0m \u001B[43m    \u001B[49m\u001B[43mamsgrad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mamsgrad\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    941\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhas_complex\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhas_complex\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    942\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbeta1\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta1\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    943\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbeta2\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta2\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    944\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlr\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    945\u001B[0m \u001B[43m    \u001B[49m\u001B[43mweight_decay\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mweight_decay\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    946\u001B[0m \u001B[43m    \u001B[49m\u001B[43meps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43meps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    947\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmaximize\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmaximize\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    948\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcapturable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcapturable\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    949\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdifferentiable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdifferentiable\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    950\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_scale\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgrad_scale\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    951\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfound_inf\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfound_inf\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    952\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdecoupled_weight_decay\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdecoupled_weight_decay\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    953\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\optim\\adam.py:738\u001B[0m, in \u001B[0;36m_multi_tensor_adam\u001B[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, decoupled_weight_decay)\u001B[0m\n\u001B[0;32m    736\u001B[0m     torch\u001B[38;5;241m.\u001B[39m_foreach_addcdiv_(device_params, device_exp_avgs, exp_avg_sq_sqrt)\n\u001B[0;32m    737\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 738\u001B[0m     bias_correction1 \u001B[38;5;241m=\u001B[39m [\n\u001B[0;32m    739\u001B[0m         \u001B[38;5;241m1\u001B[39m \u001B[38;5;241m-\u001B[39m beta1 \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m _get_value(step) \u001B[38;5;28;01mfor\u001B[39;00m step \u001B[38;5;129;01min\u001B[39;00m device_state_steps\n\u001B[0;32m    740\u001B[0m     ]\n\u001B[0;32m    741\u001B[0m     bias_correction2 \u001B[38;5;241m=\u001B[39m [\n\u001B[0;32m    742\u001B[0m         \u001B[38;5;241m1\u001B[39m \u001B[38;5;241m-\u001B[39m beta2 \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m _get_value(step) \u001B[38;5;28;01mfor\u001B[39;00m step \u001B[38;5;129;01min\u001B[39;00m device_state_steps\n\u001B[0;32m    743\u001B[0m     ]\n\u001B[0;32m    745\u001B[0m     step_size \u001B[38;5;241m=\u001B[39m _stack_if_compiling([(lr \u001B[38;5;241m/\u001B[39m bc) \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m bc \u001B[38;5;129;01min\u001B[39;00m bias_correction1])\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\optim\\adam.py:739\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m    736\u001B[0m     torch\u001B[38;5;241m.\u001B[39m_foreach_addcdiv_(device_params, device_exp_avgs, exp_avg_sq_sqrt)\n\u001B[0;32m    737\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    738\u001B[0m     bias_correction1 \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m--> 739\u001B[0m         \u001B[38;5;241m1\u001B[39m \u001B[38;5;241m-\u001B[39m beta1 \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m \u001B[43m_get_value\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstep\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m step \u001B[38;5;129;01min\u001B[39;00m device_state_steps\n\u001B[0;32m    740\u001B[0m     ]\n\u001B[0;32m    741\u001B[0m     bias_correction2 \u001B[38;5;241m=\u001B[39m [\n\u001B[0;32m    742\u001B[0m         \u001B[38;5;241m1\u001B[39m \u001B[38;5;241m-\u001B[39m beta2 \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m _get_value(step) \u001B[38;5;28;01mfor\u001B[39;00m step \u001B[38;5;129;01min\u001B[39;00m device_state_steps\n\u001B[0;32m    743\u001B[0m     ]\n\u001B[0;32m    745\u001B[0m     step_size \u001B[38;5;241m=\u001B[39m _stack_if_compiling([(lr \u001B[38;5;241m/\u001B[39m bc) \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m bc \u001B[38;5;129;01min\u001B[39;00m bias_correction1])\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\optim\\optimizer.py:94\u001B[0m, in \u001B[0;36m_get_value\u001B[1;34m(x)\u001B[0m\n\u001B[0;32m     92\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m x\n\u001B[0;32m     93\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m---> 94\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mitem\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(x, torch\u001B[38;5;241m.\u001B[39mTensor) \u001B[38;5;28;01melse\u001B[39;00m x\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## DeepMind\n",
    "\n",
    "- Se carga el modelo luego de 5M de pasos con más contexto"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-06-18T19:57:16.696816Z"
    }
   },
   "cell_type": "code",
   "source": [
    "old_model = DQN.load(\"models/deep_mind_finetuning.zip\", env=env)\n",
    "\n",
    "policy_kwargs = dict(\n",
    "    net_arch = [256, 256],\n",
    ")\n",
    "\n",
    "total_timesteps = 5_000_000\n",
    "progress_bar_callback = TQDMProgressCallback(total_timesteps=total_timesteps)\n",
    "ml_callback = MLflowCallback(\n",
    "    best_model_path=\"models/deep_mind_finetuning_more.zip\",\n",
    "    experiment_name=\"DQN_SpaceInvaders\",\n",
    "    run_name=\"DQN_Run\",\n",
    "    log_freq=25000\n",
    ")\n",
    "\n",
    "experiment_name = \"DQN_SpaceInvaders\"\n",
    "exist_experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "if not exist_experiment:\n",
    "    mlflow.create_experiment(experiment_name)\n",
    "mlflow.set_experiment(experiment_name)\n",
    "with mlflow.start_run(run_name=\"DQN_Run_deep_mind_finetuned_more\"):\n",
    "    mlflow.log_param(\"env_name\", env_name)\n",
    "    mlflow.log_param(\"total_timesteps\", total_timesteps)\n",
    "    mlflow.log_param(\"learning_rate\", 1e-3)\n",
    "    mlflow.log_param(\"buffer_size\", 1_000_000)\n",
    "    mlflow.log_param(\"exploration_fraction\", 0.2)\n",
    "    mlflow.log_param(\"exploration_final_eps\", 0.01)\n",
    "    mlflow.log_param(\"exploration_initial_eps\", 1.0)\n",
    "    mlflow.log_param(\"batch_size\", 32)\n",
    "    mlflow.log_param(\"gamma\", 0.99)\n",
    "    mlflow.log_param(\"learning_starts\", 100_000)\n",
    "    mlflow.log_param(\"target_update_interval\", 1_000)\n",
    "\n",
    "    model = DQN(\"CnnPolicy\", env, verbose=0,\n",
    "                policy_kwargs=policy_kwargs,\n",
    "                learning_rate=1e-3, buffer_size=1_000_000,\n",
    "                exploration_fraction=0.1, exploration_initial_eps=0.05,\n",
    "                exploration_final_eps=0.01, target_update_interval=1000,\n",
    "                batch_size=32, learning_starts=100_000, gamma=0.99,seed=23)\n",
    "    model.policy.load_state_dict(old_model.policy.state_dict())\n",
    "\n",
    "    t_model = model.learn(total_timesteps=total_timesteps, callback=[progress_bar_callback, ml_callback, TestCallBack(env, test_timesteps=50_000)])\n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Training Progress:   0%|          | 0/5000000 [00:00<?, ?step/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "20cd2742df85491f99abab27e3255c0a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### PPO"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-21T02:54:29.487889Z",
     "start_time": "2025-06-21T01:31:18.419298Z"
    }
   },
   "cell_type": "code",
   "source": [
    "policy_kwargs = dict(\n",
    "    features_extractor_class=DeepMindCNN,\n",
    "    features_extractor_kwargs=dict(features_dim=512),\n",
    "    net_arch=[512, 512, 216, 102, 51],\n",
    ")\n",
    "\n",
    "total_timesteps = 5_000_000\n",
    "progress_bar_callback = TQDMProgressCallback(total_timesteps=total_timesteps)\n",
    "ml_callback = MLflowCallback(\n",
    "    best_model_path=\"models/deep_mind_PPO_v2_pen.zip\",\n",
    "    experiment_name=\"DQN_SpaceInvaders\",\n",
    "    run_name=\"DQN_Run\",\n",
    "    log_freq=25000\n",
    ")\n",
    "\n",
    "experiment_name = \"DQN_SpaceInvaders\"\n",
    "exist_experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "if not exist_experiment:\n",
    "    mlflow.create_experiment(experiment_name)\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "with mlflow.start_run(run_name=\"PPO_Run_DeepMind_v2_pen\"):\n",
    "    mlflow.log_param(\"env_name\", env_name)\n",
    "    mlflow.log_param(\"total_timesteps\", total_timesteps)\n",
    "    mlflow.log_param(\"learning_rate\", 25e-4)\n",
    "    mlflow.log_param(\"n_steps\", 2048 * 3)\n",
    "    mlflow.log_param(\"frame_skip\", 12)\n",
    "\n",
    "    # model = PPO(\"CnnPolicy\", env, verbose=0, learning_rate=25e-4, policy_kwargs=policy_kwargs, n_steps=2048 * 3, seed=23)\n",
    "    model = PPO.load(\"models/deep_mind_PPO_v2_pen.zip\", env=env)\n",
    "\n",
    "    t_model = model.learn(total_timesteps=total_timesteps, callback=[progress_bar_callback, ml_callback, TestCallBack(normal_env, test_timesteps=100_000)])"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Training Progress:   0%|          | 0/5000000 [00:00<?, ?step/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a0e45b7be4834111bcc1153f66688350"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[8], line 32\u001B[0m\n\u001B[0;32m     29\u001B[0m \u001B[38;5;66;03m# model = PPO(\"CnnPolicy\", env, verbose=0, learning_rate=25e-4, policy_kwargs=policy_kwargs, n_steps=2048 * 3, seed=23)\u001B[39;00m\n\u001B[0;32m     30\u001B[0m model \u001B[38;5;241m=\u001B[39m PPO\u001B[38;5;241m.\u001B[39mload(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodels/deep_mind_PPO_v2_pen.zip\u001B[39m\u001B[38;5;124m\"\u001B[39m, env\u001B[38;5;241m=\u001B[39menv)\n\u001B[1;32m---> 32\u001B[0m t_model \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlearn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtotal_timesteps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtotal_timesteps\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallback\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[43mprogress_bar_callback\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mml_callback\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mTestCallBack\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnormal_env\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_timesteps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m100_000\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:311\u001B[0m, in \u001B[0;36mPPO.learn\u001B[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001B[0m\n\u001B[0;32m    302\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mlearn\u001B[39m(\n\u001B[0;32m    303\u001B[0m     \u001B[38;5;28mself\u001B[39m: SelfPPO,\n\u001B[0;32m    304\u001B[0m     total_timesteps: \u001B[38;5;28mint\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    309\u001B[0m     progress_bar: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m    310\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m SelfPPO:\n\u001B[1;32m--> 311\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlearn\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    312\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtotal_timesteps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtotal_timesteps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    313\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcallback\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcallback\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    314\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlog_interval\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlog_interval\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    315\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtb_log_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtb_log_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    316\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreset_num_timesteps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreset_num_timesteps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    317\u001B[0m \u001B[43m        \u001B[49m\u001B[43mprogress_bar\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mprogress_bar\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    318\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:324\u001B[0m, in \u001B[0;36mOnPolicyAlgorithm.learn\u001B[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001B[0m\n\u001B[0;32m    321\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menv \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    323\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_timesteps \u001B[38;5;241m<\u001B[39m total_timesteps:\n\u001B[1;32m--> 324\u001B[0m     continue_training \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcollect_rollouts\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallback\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrollout_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_rollout_steps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mn_steps\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    326\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m continue_training:\n\u001B[0;32m    327\u001B[0m         \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:202\u001B[0m, in \u001B[0;36mOnPolicyAlgorithm.collect_rollouts\u001B[1;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001B[0m\n\u001B[0;32m    199\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m th\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[0;32m    200\u001B[0m     \u001B[38;5;66;03m# Convert to pytorch tensor or to TensorDict\u001B[39;00m\n\u001B[0;32m    201\u001B[0m     obs_tensor \u001B[38;5;241m=\u001B[39m obs_as_tensor(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_last_obs, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[1;32m--> 202\u001B[0m     actions, values, log_probs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpolicy\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobs_tensor\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    203\u001B[0m actions \u001B[38;5;241m=\u001B[39m actions\u001B[38;5;241m.\u001B[39mcpu()\u001B[38;5;241m.\u001B[39mnumpy()\n\u001B[0;32m    205\u001B[0m \u001B[38;5;66;03m# Rescale and perform action\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1749\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1751\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1757\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1758\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1759\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1760\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1761\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1762\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1764\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1765\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\stable_baselines3\\common\\policies.py:645\u001B[0m, in \u001B[0;36mActorCriticPolicy.forward\u001B[1;34m(self, obs, deterministic)\u001B[0m\n\u001B[0;32m    637\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    638\u001B[0m \u001B[38;5;124;03mForward pass in all the networks (actor and critic)\u001B[39;00m\n\u001B[0;32m    639\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    642\u001B[0m \u001B[38;5;124;03m:return: action, value and log probability of the action\u001B[39;00m\n\u001B[0;32m    643\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    644\u001B[0m \u001B[38;5;66;03m# Preprocess the observation if needed\u001B[39;00m\n\u001B[1;32m--> 645\u001B[0m features \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mextract_features\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    646\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mshare_features_extractor:\n\u001B[0;32m    647\u001B[0m     latent_pi, latent_vf \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmlp_extractor(features)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\stable_baselines3\\common\\policies.py:672\u001B[0m, in \u001B[0;36mActorCriticPolicy.extract_features\u001B[1;34m(self, obs, features_extractor)\u001B[0m\n\u001B[0;32m    663\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    664\u001B[0m \u001B[38;5;124;03mPreprocess the observation if needed and extract features.\u001B[39;00m\n\u001B[0;32m    665\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    669\u001B[0m \u001B[38;5;124;03m    features for the actor and the features for the critic.\u001B[39;00m\n\u001B[0;32m    670\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    671\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mshare_features_extractor:\n\u001B[1;32m--> 672\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mextract_features\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfeatures_extractor\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mfeatures_extractor\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mis\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mfeatures_extractor\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    673\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    674\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m features_extractor \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\stable_baselines3\\common\\policies.py:131\u001B[0m, in \u001B[0;36mBaseModel.extract_features\u001B[1;34m(self, obs, features_extractor)\u001B[0m\n\u001B[0;32m    123\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    124\u001B[0m \u001B[38;5;124;03mPreprocess the observation if needed and extract features.\u001B[39;00m\n\u001B[0;32m    125\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    128\u001B[0m \u001B[38;5;124;03m:return: The extracted features\u001B[39;00m\n\u001B[0;32m    129\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    130\u001B[0m preprocessed_obs \u001B[38;5;241m=\u001B[39m preprocess_obs(obs, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mobservation_space, normalize_images\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnormalize_images)\n\u001B[1;32m--> 131\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfeatures_extractor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpreprocessed_obs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1749\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1751\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1757\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1758\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1759\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1760\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1761\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1762\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1764\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1765\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_3604\\2760391963.py:47\u001B[0m, in \u001B[0;36mforward\u001B[1;34m(self, observations)\u001B[0m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1749\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1751\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1757\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1758\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1759\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1760\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1761\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1762\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1764\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1765\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\container.py:240\u001B[0m, in \u001B[0;36mSequential.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    238\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[0;32m    239\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[1;32m--> 240\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    241\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1749\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1751\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1757\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1758\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1759\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1760\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1761\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1762\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1764\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1765\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001B[0m, in \u001B[0;36mLinear.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    124\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 125\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### A2C"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-21T22:24:12.483239Z",
     "start_time": "2025-06-21T21:03:36.735074Z"
    }
   },
   "cell_type": "code",
   "source": [
    "policy_kwargs = dict(\n",
    "    features_extractor_class=DeepMindCNN,\n",
    "    features_extractor_kwargs=dict(features_dim=512),\n",
    "    net_arch=dict(pi=[216, 216, 216], vf=[216, 216, 216])  # pi: policy, vf: value\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "total_timesteps = 5_000_000\n",
    "progress_bar_callback = TQDMProgressCallback(total_timesteps=total_timesteps)\n",
    "ml_callback = MLflowCallback(\n",
    "    best_model_path=\"models/deep_mind_A2C.zip\",\n",
    "    experiment_name=\"DQN_SpaceInvaders\",\n",
    "    run_name=\"DQN_Run\",\n",
    "    log_freq=25000\n",
    ")\n",
    "\n",
    "experiment_name = \"DQN_SpaceInvaders\"\n",
    "exist_experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "if not exist_experiment:\n",
    "    mlflow.create_experiment(experiment_name)\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "\n",
    "with mlflow.start_run(run_name=\"PPO_Run_DeepMind_A2C\"):\n",
    "    mlflow.log_param(\"env_name\", env_name)\n",
    "    mlflow.log_param(\"total_timesteps\", total_timesteps)\n",
    "    mlflow.log_param(\"learning_rate\", 7e-4)\n",
    "    mlflow.log_param(\"frame_skip\", 4)\n",
    "    mlflow.log_param(\"gamma\", 0.95)\n",
    "    mlflow.log_param(\"stats_window_size\", 1_000)\n",
    "    mlflow.log_param(\"ent_coef\", 0.05)\n",
    "\n",
    "    model = A2C(\"CnnPolicy\", normal_env, stats_window_size=1000,verbose=0, ent_coef=0.05, gamma=0.95, seed=23, policy_kwargs=policy_kwargs, device=\"cuda\")\n",
    "\n",
    "    t_model = model.learn(total_timesteps=total_timesteps, callback=[progress_bar_callback, ml_callback, TestCallBack(normal_env, test_timesteps=100_000)])\n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Training Progress:   0%|          | 0/5000000 [00:00<?, ?step/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "51806bd7e508455795587585b806787d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\migue\\AppData\\Local\\Temp\\ipykernel_29316\\1002412995.py:21: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  action_scalar = int(action)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[18], line 36\u001B[0m\n\u001B[0;32m     32\u001B[0m mlflow\u001B[38;5;241m.\u001B[39mlog_param(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124ment_coef\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;241m0.05\u001B[39m)\n\u001B[0;32m     34\u001B[0m model \u001B[38;5;241m=\u001B[39m A2C(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCnnPolicy\u001B[39m\u001B[38;5;124m\"\u001B[39m, normal_env, stats_window_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1000\u001B[39m,verbose\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m, ent_coef\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.05\u001B[39m, gamma\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.95\u001B[39m, seed\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m23\u001B[39m, policy_kwargs\u001B[38;5;241m=\u001B[39mpolicy_kwargs, device\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcuda\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m---> 36\u001B[0m t_model \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlearn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtotal_timesteps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtotal_timesteps\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallback\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[43mprogress_bar_callback\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mml_callback\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mTestCallBack\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnormal_env\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_timesteps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m100_000\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\stable_baselines3\\a2c\\a2c.py:201\u001B[0m, in \u001B[0;36mA2C.learn\u001B[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001B[0m\n\u001B[0;32m    192\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mlearn\u001B[39m(\n\u001B[0;32m    193\u001B[0m     \u001B[38;5;28mself\u001B[39m: SelfA2C,\n\u001B[0;32m    194\u001B[0m     total_timesteps: \u001B[38;5;28mint\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    199\u001B[0m     progress_bar: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m    200\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m SelfA2C:\n\u001B[1;32m--> 201\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlearn\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    202\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtotal_timesteps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtotal_timesteps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    203\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcallback\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcallback\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    204\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlog_interval\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlog_interval\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    205\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtb_log_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtb_log_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    206\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreset_num_timesteps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreset_num_timesteps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    207\u001B[0m \u001B[43m        \u001B[49m\u001B[43mprogress_bar\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mprogress_bar\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    208\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:324\u001B[0m, in \u001B[0;36mOnPolicyAlgorithm.learn\u001B[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001B[0m\n\u001B[0;32m    321\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menv \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    323\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_timesteps \u001B[38;5;241m<\u001B[39m total_timesteps:\n\u001B[1;32m--> 324\u001B[0m     continue_training \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcollect_rollouts\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallback\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrollout_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_rollout_steps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mn_steps\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    326\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m continue_training:\n\u001B[0;32m    327\u001B[0m         \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:218\u001B[0m, in \u001B[0;36mOnPolicyAlgorithm.collect_rollouts\u001B[1;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001B[0m\n\u001B[0;32m    213\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    214\u001B[0m         \u001B[38;5;66;03m# Otherwise, clip the actions to avoid out of bound error\u001B[39;00m\n\u001B[0;32m    215\u001B[0m         \u001B[38;5;66;03m# as we are sampling from an unbounded Gaussian distribution\u001B[39;00m\n\u001B[0;32m    216\u001B[0m         clipped_actions \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mclip(actions, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maction_space\u001B[38;5;241m.\u001B[39mlow, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maction_space\u001B[38;5;241m.\u001B[39mhigh)\n\u001B[1;32m--> 218\u001B[0m new_obs, rewards, dones, infos \u001B[38;5;241m=\u001B[39m \u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43mclipped_actions\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    220\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_timesteps \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m env\u001B[38;5;241m.\u001B[39mnum_envs\n\u001B[0;32m    222\u001B[0m \u001B[38;5;66;03m# Give access to local variables\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\base_vec_env.py:222\u001B[0m, in \u001B[0;36mVecEnv.step\u001B[1;34m(self, actions)\u001B[0m\n\u001B[0;32m    215\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    216\u001B[0m \u001B[38;5;124;03mStep the environments with the given action\u001B[39;00m\n\u001B[0;32m    217\u001B[0m \n\u001B[0;32m    218\u001B[0m \u001B[38;5;124;03m:param actions: the action\u001B[39;00m\n\u001B[0;32m    219\u001B[0m \u001B[38;5;124;03m:return: observation, reward, done, information\u001B[39;00m\n\u001B[0;32m    220\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    221\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstep_async(actions)\n\u001B[1;32m--> 222\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep_wait\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\vec_frame_stack.py:39\u001B[0m, in \u001B[0;36mVecFrameStack.step_wait\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     31\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mstep_wait\u001B[39m(\n\u001B[0;32m     32\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m     33\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mtuple\u001B[39m[\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     37\u001B[0m     \u001B[38;5;28mlist\u001B[39m[\u001B[38;5;28mdict\u001B[39m[\u001B[38;5;28mstr\u001B[39m, Any]],\n\u001B[0;32m     38\u001B[0m ]:\n\u001B[1;32m---> 39\u001B[0m     observations, rewards, dones, infos \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvenv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep_wait\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     40\u001B[0m     observations, infos \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstacked_obs\u001B[38;5;241m.\u001B[39mupdate(observations, dones, infos)  \u001B[38;5;66;03m# type: ignore[arg-type]\u001B[39;00m\n\u001B[0;32m     41\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m observations, rewards, dones, infos\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\dummy_vec_env.py:59\u001B[0m, in \u001B[0;36mDummyVecEnv.step_wait\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     56\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mstep_wait\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m VecEnvStepReturn:\n\u001B[0;32m     57\u001B[0m     \u001B[38;5;66;03m# Avoid circular imports\u001B[39;00m\n\u001B[0;32m     58\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m env_idx \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_envs):\n\u001B[1;32m---> 59\u001B[0m         obs, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbuf_rews[env_idx], terminated, truncated, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbuf_infos[env_idx] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menvs\u001B[49m\u001B[43m[\u001B[49m\u001B[43menv_idx\u001B[49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# type: ignore[assignment]\u001B[39;49;00m\n\u001B[0;32m     60\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mactions\u001B[49m\u001B[43m[\u001B[49m\u001B[43menv_idx\u001B[49m\u001B[43m]\u001B[49m\n\u001B[0;32m     61\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     62\u001B[0m         \u001B[38;5;66;03m# convert to SB3 VecEnv api\u001B[39;00m\n\u001B[0;32m     63\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbuf_dones[env_idx] \u001B[38;5;241m=\u001B[39m terminated \u001B[38;5;129;01mor\u001B[39;00m truncated\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\stable_baselines3\\common\\monitor.py:94\u001B[0m, in \u001B[0;36mMonitor.step\u001B[1;34m(self, action)\u001B[0m\n\u001B[0;32m     92\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mneeds_reset:\n\u001B[0;32m     93\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTried to step environment that needs reset\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m---> 94\u001B[0m observation, reward, terminated, truncated, info \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     95\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrewards\u001B[38;5;241m.\u001B[39mappend(\u001B[38;5;28mfloat\u001B[39m(reward))\n\u001B[0;32m     96\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m terminated \u001B[38;5;129;01mor\u001B[39;00m truncated:\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\gymnasium\\core.py:560\u001B[0m, in \u001B[0;36mObservationWrapper.step\u001B[1;34m(self, action)\u001B[0m\n\u001B[0;32m    556\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mstep\u001B[39m(\n\u001B[0;32m    557\u001B[0m     \u001B[38;5;28mself\u001B[39m, action: ActType\n\u001B[0;32m    558\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mtuple\u001B[39m[WrapperObsType, SupportsFloat, \u001B[38;5;28mbool\u001B[39m, \u001B[38;5;28mbool\u001B[39m, \u001B[38;5;28mdict\u001B[39m[\u001B[38;5;28mstr\u001B[39m, Any]]:\n\u001B[0;32m    559\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Modifies the :attr:`env` after calling :meth:`step` using :meth:`self.observation` on the returned observations.\"\"\"\u001B[39;00m\n\u001B[1;32m--> 560\u001B[0m     observation, reward, terminated, truncated, info \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    561\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mobservation(observation), reward, terminated, truncated, info\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\gymnasium\\core.py:327\u001B[0m, in \u001B[0;36mWrapper.step\u001B[1;34m(self, action)\u001B[0m\n\u001B[0;32m    323\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mstep\u001B[39m(\n\u001B[0;32m    324\u001B[0m     \u001B[38;5;28mself\u001B[39m, action: WrapperActType\n\u001B[0;32m    325\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mtuple\u001B[39m[WrapperObsType, SupportsFloat, \u001B[38;5;28mbool\u001B[39m, \u001B[38;5;28mbool\u001B[39m, \u001B[38;5;28mdict\u001B[39m[\u001B[38;5;28mstr\u001B[39m, Any]]:\n\u001B[0;32m    326\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Uses the :meth:`step` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001B[39;00m\n\u001B[1;32m--> 327\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\gymnasium\\core.py:595\u001B[0m, in \u001B[0;36mRewardWrapper.step\u001B[1;34m(self, action)\u001B[0m\n\u001B[0;32m    591\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mstep\u001B[39m(\n\u001B[0;32m    592\u001B[0m     \u001B[38;5;28mself\u001B[39m, action: ActType\n\u001B[0;32m    593\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mtuple\u001B[39m[ObsType, SupportsFloat, \u001B[38;5;28mbool\u001B[39m, \u001B[38;5;28mbool\u001B[39m, \u001B[38;5;28mdict\u001B[39m[\u001B[38;5;28mstr\u001B[39m, Any]]:\n\u001B[0;32m    594\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Modifies the :attr:`env` :meth:`step` reward using :meth:`self.reward`.\"\"\"\u001B[39;00m\n\u001B[1;32m--> 595\u001B[0m     observation, reward, terminated, truncated, info \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    596\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m observation, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreward(reward), terminated, truncated, info\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\gymnasium\\core.py:560\u001B[0m, in \u001B[0;36mObservationWrapper.step\u001B[1;34m(self, action)\u001B[0m\n\u001B[0;32m    556\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mstep\u001B[39m(\n\u001B[0;32m    557\u001B[0m     \u001B[38;5;28mself\u001B[39m, action: ActType\n\u001B[0;32m    558\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mtuple\u001B[39m[WrapperObsType, SupportsFloat, \u001B[38;5;28mbool\u001B[39m, \u001B[38;5;28mbool\u001B[39m, \u001B[38;5;28mdict\u001B[39m[\u001B[38;5;28mstr\u001B[39m, Any]]:\n\u001B[0;32m    559\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Modifies the :attr:`env` after calling :meth:`step` using :meth:`self.observation` on the returned observations.\"\"\"\u001B[39;00m\n\u001B[1;32m--> 560\u001B[0m     observation, reward, terminated, truncated, info \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    561\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mobservation(observation), reward, terminated, truncated, info\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\gymnasium\\core.py:327\u001B[0m, in \u001B[0;36mWrapper.step\u001B[1;34m(self, action)\u001B[0m\n\u001B[0;32m    323\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mstep\u001B[39m(\n\u001B[0;32m    324\u001B[0m     \u001B[38;5;28mself\u001B[39m, action: WrapperActType\n\u001B[0;32m    325\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mtuple\u001B[39m[WrapperObsType, SupportsFloat, \u001B[38;5;28mbool\u001B[39m, \u001B[38;5;28mbool\u001B[39m, \u001B[38;5;28mdict\u001B[39m[\u001B[38;5;28mstr\u001B[39m, Any]]:\n\u001B[0;32m    326\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Uses the :meth:`step` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001B[39;00m\n\u001B[1;32m--> 327\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\stable_baselines3\\common\\atari_wrappers.py:112\u001B[0m, in \u001B[0;36mEpisodicLifeEnv.step\u001B[1;34m(self, action)\u001B[0m\n\u001B[0;32m    111\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mstep\u001B[39m(\u001B[38;5;28mself\u001B[39m, action: \u001B[38;5;28mint\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m AtariStepReturn:\n\u001B[1;32m--> 112\u001B[0m     obs, reward, terminated, truncated, info \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    113\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mwas_real_done \u001B[38;5;241m=\u001B[39m terminated \u001B[38;5;129;01mor\u001B[39;00m truncated\n\u001B[0;32m    114\u001B[0m     \u001B[38;5;66;03m# check current lives, make loss of life terminal,\u001B[39;00m\n\u001B[0;32m    115\u001B[0m     \u001B[38;5;66;03m# then update lives to handle bonus lives\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\stable_baselines3\\common\\atari_wrappers.py:184\u001B[0m, in \u001B[0;36mMaxAndSkipEnv.step\u001B[1;34m(self, action)\u001B[0m\n\u001B[0;32m    182\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m i \u001B[38;5;241m==\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_skip \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m    183\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_obs_buffer[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m=\u001B[39m obs\n\u001B[1;32m--> 184\u001B[0m total_reward \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mfloat\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mreward\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    185\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m done:\n\u001B[0;32m    186\u001B[0m     \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-20T17:10:56.736116Z",
     "start_time": "2025-06-20T14:21:44.787566Z"
    }
   },
   "cell_type": "code",
   "source": [
    "policy_kwargs = dict(\n",
    "    net_arch=[512, 512, 216, 102, 51],\n",
    ")\n",
    "\n",
    "old_model = PPO.load(\"models/deep_mind_PPO.zip\", env=env)\n",
    "\n",
    "total_timesteps = 5_000_000\n",
    "progress_bar_callback = TQDMProgressCallback(total_timesteps=total_timesteps)\n",
    "ml_callback = MLflowCallback(\n",
    "    best_model_path=\"models/deep_mind_PPO_enhance.zip\",\n",
    "    experiment_name=\"DQN_SpaceInvaders\",\n",
    "    run_name=\"DQN_Run\",\n",
    "    log_freq=25000\n",
    ")\n",
    "\n",
    "experiment_name = \"DQN_SpaceInvaders\"\n",
    "exist_experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "if not exist_experiment:\n",
    "    mlflow.create_experiment(experiment_name)\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "with mlflow.start_run(run_name=\"PPO_Run_DeepMind\"):\n",
    "    mlflow.log_param(\"env_name\", env_name)\n",
    "    mlflow.log_param(\"total_timesteps\", total_timesteps)\n",
    "    mlflow.log_param(\"learning_rate\", 25e-5)\n",
    "\n",
    "    model = PPO(\"CnnPolicy\", env, verbose=0, n_steps=2048 * 3,learning_rate=25e-5, clip_range=0.1, ent_coef=0.01, policy_kwargs=policy_kwargs, seed=23)\n",
    "    model.policy.load_state_dict(old_model.policy.state_dict())\n",
    "\n",
    "    t_model = model.learn(total_timesteps=total_timesteps, callback=[progress_bar_callback, ml_callback, TestCallBack(env, test_timesteps=100_000)], reset_num_timesteps=True)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Training Progress:   0%|          | 0/5000000 [00:00<?, ?step/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f038d39332e64965849af1a7085651ea"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[23], line 30\u001B[0m\n\u001B[0;32m     27\u001B[0m model \u001B[38;5;241m=\u001B[39m PPO(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCnnPolicy\u001B[39m\u001B[38;5;124m\"\u001B[39m, env, verbose\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m, n_steps\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2048\u001B[39m \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m3\u001B[39m,learning_rate\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m25e-5\u001B[39m, clip_range\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.1\u001B[39m, ent_coef\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.01\u001B[39m, policy_kwargs\u001B[38;5;241m=\u001B[39mpolicy_kwargs, seed\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m23\u001B[39m)\n\u001B[0;32m     28\u001B[0m model\u001B[38;5;241m.\u001B[39mpolicy\u001B[38;5;241m.\u001B[39mload_state_dict(old_model\u001B[38;5;241m.\u001B[39mpolicy\u001B[38;5;241m.\u001B[39mstate_dict())\n\u001B[1;32m---> 30\u001B[0m t_model \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlearn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtotal_timesteps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtotal_timesteps\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallback\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[43mprogress_bar_callback\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mml_callback\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mTestCallBack\u001B[49m\u001B[43m(\u001B[49m\u001B[43menv\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_timesteps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m100_000\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreset_num_timesteps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:311\u001B[0m, in \u001B[0;36mPPO.learn\u001B[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001B[0m\n\u001B[0;32m    302\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mlearn\u001B[39m(\n\u001B[0;32m    303\u001B[0m     \u001B[38;5;28mself\u001B[39m: SelfPPO,\n\u001B[0;32m    304\u001B[0m     total_timesteps: \u001B[38;5;28mint\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    309\u001B[0m     progress_bar: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m    310\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m SelfPPO:\n\u001B[1;32m--> 311\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlearn\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    312\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtotal_timesteps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtotal_timesteps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    313\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcallback\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcallback\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    314\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlog_interval\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlog_interval\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    315\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtb_log_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtb_log_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    316\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreset_num_timesteps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreset_num_timesteps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    317\u001B[0m \u001B[43m        \u001B[49m\u001B[43mprogress_bar\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mprogress_bar\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    318\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:324\u001B[0m, in \u001B[0;36mOnPolicyAlgorithm.learn\u001B[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001B[0m\n\u001B[0;32m    321\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menv \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    323\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_timesteps \u001B[38;5;241m<\u001B[39m total_timesteps:\n\u001B[1;32m--> 324\u001B[0m     continue_training \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcollect_rollouts\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallback\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrollout_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_rollout_steps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mn_steps\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    326\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m continue_training:\n\u001B[0;32m    327\u001B[0m         \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:202\u001B[0m, in \u001B[0;36mOnPolicyAlgorithm.collect_rollouts\u001B[1;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001B[0m\n\u001B[0;32m    199\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m th\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[0;32m    200\u001B[0m     \u001B[38;5;66;03m# Convert to pytorch tensor or to TensorDict\u001B[39;00m\n\u001B[0;32m    201\u001B[0m     obs_tensor \u001B[38;5;241m=\u001B[39m obs_as_tensor(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_last_obs, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[1;32m--> 202\u001B[0m     actions, values, log_probs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpolicy\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobs_tensor\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    203\u001B[0m actions \u001B[38;5;241m=\u001B[39m actions\u001B[38;5;241m.\u001B[39mcpu()\u001B[38;5;241m.\u001B[39mnumpy()\n\u001B[0;32m    205\u001B[0m \u001B[38;5;66;03m# Rescale and perform action\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1749\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1751\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1757\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1758\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1759\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1760\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1761\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1762\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1764\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1765\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\stable_baselines3\\common\\policies.py:645\u001B[0m, in \u001B[0;36mActorCriticPolicy.forward\u001B[1;34m(self, obs, deterministic)\u001B[0m\n\u001B[0;32m    637\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    638\u001B[0m \u001B[38;5;124;03mForward pass in all the networks (actor and critic)\u001B[39;00m\n\u001B[0;32m    639\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    642\u001B[0m \u001B[38;5;124;03m:return: action, value and log probability of the action\u001B[39;00m\n\u001B[0;32m    643\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    644\u001B[0m \u001B[38;5;66;03m# Preprocess the observation if needed\u001B[39;00m\n\u001B[1;32m--> 645\u001B[0m features \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mextract_features\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    646\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mshare_features_extractor:\n\u001B[0;32m    647\u001B[0m     latent_pi, latent_vf \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmlp_extractor(features)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\stable_baselines3\\common\\policies.py:672\u001B[0m, in \u001B[0;36mActorCriticPolicy.extract_features\u001B[1;34m(self, obs, features_extractor)\u001B[0m\n\u001B[0;32m    663\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    664\u001B[0m \u001B[38;5;124;03mPreprocess the observation if needed and extract features.\u001B[39;00m\n\u001B[0;32m    665\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    669\u001B[0m \u001B[38;5;124;03m    features for the actor and the features for the critic.\u001B[39;00m\n\u001B[0;32m    670\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    671\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mshare_features_extractor:\n\u001B[1;32m--> 672\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mextract_features\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfeatures_extractor\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mfeatures_extractor\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mis\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mfeatures_extractor\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    673\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    674\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m features_extractor \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\stable_baselines3\\common\\policies.py:131\u001B[0m, in \u001B[0;36mBaseModel.extract_features\u001B[1;34m(self, obs, features_extractor)\u001B[0m\n\u001B[0;32m    123\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    124\u001B[0m \u001B[38;5;124;03mPreprocess the observation if needed and extract features.\u001B[39;00m\n\u001B[0;32m    125\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    128\u001B[0m \u001B[38;5;124;03m:return: The extracted features\u001B[39;00m\n\u001B[0;32m    129\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    130\u001B[0m preprocessed_obs \u001B[38;5;241m=\u001B[39m preprocess_obs(obs, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mobservation_space, normalize_images\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnormalize_images)\n\u001B[1;32m--> 131\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfeatures_extractor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpreprocessed_obs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1749\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1751\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1757\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1758\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1759\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1760\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1761\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1762\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1764\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1765\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\stable_baselines3\\common\\torch_layers.py:107\u001B[0m, in \u001B[0;36mNatureCNN.forward\u001B[1;34m(self, observations)\u001B[0m\n\u001B[0;32m    106\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, observations: th\u001B[38;5;241m.\u001B[39mTensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m th\u001B[38;5;241m.\u001B[39mTensor:\n\u001B[1;32m--> 107\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlinear(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcnn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobservations\u001B[49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1749\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1751\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1757\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1758\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1759\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1760\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1761\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1762\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1764\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1765\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\container.py:240\u001B[0m, in \u001B[0;36mSequential.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    238\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[0;32m    239\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[1;32m--> 240\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    241\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1749\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1751\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1757\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1758\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1759\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1760\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1761\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1762\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1764\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1765\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\flatten.py:53\u001B[0m, in \u001B[0;36mFlatten.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m     52\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m---> 53\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43minput\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mflatten\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstart_dim\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mend_dim\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OHYryKd1Gb2b"
   },
   "outputs": [],
   "source": [
    "# Testing part to calculate the mean reward\n",
    "weights_filename = 'dqn_{}_weights.h5f'.format(env_name)\n",
    "dqn.load_weights(weights_filename)\n",
    "dqn.test(env, nb_episodes=10, visualize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-NAlu8b1Gb2b"
   },
   "source": [
    "3. Justificación de los parámetros seleccionados y de los resultados obtenidos"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "---"
   ],
   "metadata": {
    "id": "ANFQiicXK3sO"
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch] *",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
